\documentclass{llncs}

\usepackage{xcolor}
\usepackage{enumitem,amsmath,amssymb}
\usepackage{breakurl}    % used for \url and \burl
\usepackage[linesnumbered,boxed,noline,noend]{algorithm2e}
\def\defaultHypSeparation{\hskip.1in}

\usepackage{tikz}
\usepackage{subfig}
\usepackage{array,booktabs,multirow}
\usepackage{placeins}

\usepackage{logictools}
\usepackage{prooftheory}
\usepackage{comment}
\usepackage{mathenvironments}
\usepackage{drawproof}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}

\title{Towards the Compression of First-Order Resolution Proofs by Lowering Unit Clauses}

\author{
  Jan Gorzny\inst{1}
  \thanks{Supported by the Google Summer of Code 2014 program.}
  \and 
  Bruno Woltzenlogel Paleo\inst{2}
  \thanks{Supported by the Austrian Science Fund, project P24300.}
}

\authorrunning{J.\~Gorzny \and B.\~Woltzenlogel Paleo}

\institute{
  University of Victoria, Canada \\
  \email{joseph.boudou@matabio.net}
  \and 
  Vienna University of Technology, Austria \\
  \email{bruno@logic.at}
}




\begin{document}

\maketitle


\begin{abstract}
The recently developed {\LowerUnits} algorithm compresses
propositional resolution proofs generated by SAT- and SMT-solvers by lowering (i.e. postponing) resolution inferences involving unit clauses (i.e. clauses having exactly one literal). This paper describes a generalization of this algorithm to the case of first-order resolution proofs generated by automated theorem provers. An empirical evaluation of a simplified version of this algorithm on hundreds of proofs shows promising results.
\end{abstract}


\setcounter{footnote}{0}

\section{Introduction}

ToDo:

Propositional resolution is among the most successful proof calculi for automated deduction in propositional logic available today. It provides the foundation for DPLL- and CDCL-based Sat/SMT-solvers \cite{veriT}, which perform surprisingly well in practice \cite{sat-competition}, despite the NP-completeness of propositional satisfiability \cite{cook} and the theoretical difficulty associated with NP-complete problems. 

Resolution refutations can also be output by Sat/SMT-solvers with an acceptable efficiency overhead and are detailed enough to allow easy implementation of efficient proof checkers. They can, therefore, be used as certificates of correctness for the answers provided by these tools in case of unsatisfiability.

However, as the refutations found by Sat/SMT-solvers are often redundant, techniques for
compressing and improving resolution proofs in a post-processing stage have flourished.  Algebraic properties of the resolution
operation that might be useful for compression were investigated in \cite{bwp10}.
Compression algorithms based on rearranging and sharing chains of resolution inferences have been
developed in \cite{Amjad07} and \cite{Sinz}.  Cotton \cite{CottonSplit} proposed an algorithm that
compresses a refutation by repeteadly splitting it into a proof of a heuristically chosen literal $\ell$
and a proof of $\dual{\ell}$, and then resolving them to form a new refutation.  The {\ReduceReconstruct} algorithm \cite{RedRec} searches for locally redundant
subproofs that can be rewritten into subproofs of stronger clauses and with fewer resolution steps.
In \cite{RP08} two linear time compression algorithms are introduced. One of them is a partial
regularization algorithm called {\RecyclePivots}.  An enhanced version of this latter
algorithm, called {\RecyclePivotsIntersection} ({\RPI}), is proposed in \cite{LURPI},
along with a new linear time algorithm called {\LowerUnits}.  These two last algorithms are
complementary and better compression can easily be achieved by sequentially composing them (i.e.
executing one after the other).

In this paper, the new algorithm {\LowerUnivalents}, generalizing {\LowerUnits}, is described. Its achieved
goals are to compress more than {\LowerUnits} and to allow fast \emph{non-sequential}  combination
with {\RPI}. While in a sequential combination one algorithm is simply executed after the other, in
a non-sequential combination, both algorithms are executed simultaneously when the proof is
traversed. Therefore, fewer traversals are needed.

The next section introduces the propositional resolution calculus using notations that are more convenient for describing proof transformation operations. It also describes the new concepts of \emph{active literals} and \emph{valent literals} and proves basic but essential results about them. Section \ref{sec:LU} briefly describes
the {\LowerUnits} algorithm. In Sect. \ref{sec:LUniv} the new algorithm {\LowerUnivalents} is
introduced and it is proved that it always compresses more than {\LowerUnits}. Section
\ref{sec:LUnivRPI} describes the non-sequential combination of {\LowerUnivalents} and {\RPI}.
Lastly, experimental results are discussed in Sect. \ref{sec:exp}.


\newcommand{\freevar}[1]{\mathrm{FV}(#1)}

\section{The Resolution Calculus}

We assume that there are infinitely many variable symbols (e.g. $X$, $Y$, $Z$, $X_1$, $X_2$, \ldots), constant symbols (e.g. $a$, $b$, $c$, $a_1$, $a_2$, \ldots),function symbols of every arity (e.g $f$, $g$, $f_1$, $f_2$, \ldots) and predicate symbols of every arity (e.g. $p$, $q$, $p_1$, $p_2$,\ldots). A \emph{term} is any variable, constant or the application of an $n$-ary function symbol to $n$ terms.
An \emph{atomic formula} (\emph{atom}) is the application of an $n$-ary predicate symbol to $n$ terms. A \emph{literal} is an atom or the negation of an atom. The
\emph{complement} of a literal $\ell$ is denoted $\dual{\ell}$ (i.e. for any atom $p$,
$\dual{p} = \neg p$ and $\dual{\neg p} = p$). The set of all literals is denoted $\mathcal{L}$. A
\emph{clause} is a multiset of literals. $\bot$ denotes the \emph{empty clause}.
$\freevar{t}$ (resp. $\freevar{\ell}$, $\freevar{\clause}$) denotes the set of variables in the term $t$ (resp. in the literal $\ell$ and in the clause $\clause$).
A \emph{substitution} $\{ X_1\backslash t_1, X_2 \backslash t_2, \ldots \}$ is a mapping from variables $\{ X_1, X_2, \ldots \}$ to, respectively, terms $\{t_1, t_2, \ldots \}$. The application of a substitution $\sigma$ to a term $t$, a literal $\ell$ or a clause $\clause$ results in, respectively, the term $t \sigma$, the literal $\ell \sigma$ or the clause $\clause \sigma$, obtained from $t$, $\ell$ and $\clause$ by replacing all occurrences of the variables in $\sigma$ by the corresponding terms in $\sigma$. The set of all substitutions is denoted $\mathcal{S}$. If  A \emph{unifier} of a set of literals is a substitution that makes all literals in the set equal.
A \emph{resolution proof} is a directed acyclic graph of clauses where the edges correspond to the inference rules of resolution and contraction (as explained in detail in Definition \ref{def:proof}). A \emph{resolution refutation} is a resolution proof with root $\bot$.


\newcommand{\axiom}[1]{\widehat{#1}}
\newcommand{\n}{v}
\newcommand{\raiz}[1]{\rho(#1)}

% Contraction
\newcommand{\con}[3]{\lfloor #1 \rfloor_{#2}^{#3}}

\begin{definition}[First-Order Resolution Proof] 
\label{def:proof} \hfill \\
A directed acyclic graph $\langle V, E, \clause \rangle$, where $V$ is a set of nodes and $E$ is a
set of edges labeled by literals and substitutions (i.e. $E \subset V \times \mathcal{L} \times \mathcal{S} \times V$ and $\n_1
\xrightarrow[\sigma]{\ell} \n_2$ denotes an edge from node $\n_1$ to node $\n_2$ labeled by the literal $\ell$ and the substitution $\sigma$), is a
proof of a clause $\clause$ iff it is inductively constructible according to the following cases:
%
\begin{itemize}
  \item \textbf{Axiom:} If $\Gamma$ is a clause, $\axiom{\Gamma}$ denotes some proof $\langle \{ \n \}, \varnothing,
    \Gamma \rangle$, where $\n$ is a new (axiom) node.
  \item \textbf{Resolution:} If $\psi_L$ is a proof $\langle V_L, E_L, \clause_L \rangle$ with $\ell_L \in \clause_L$ and
    $\psi_R$ is a proof $\langle V_R, E_R, \clause_R \rangle$ with $\ell_R \in \clause_R$, and 
    $\sigma_L$ and $\sigma_R$ are substitutions such that
    $\ell_L \sigma_L = \dual{\ell_R} \sigma_R$ and
    $\freevar{\left( \clause_L \setminus \left\{ \ell_L \right\} \right) \sigma_L} \cap 
     \freevar{\left( \clause_R
                    \setminus \left\{ \ell_R \right\} \right) \sigma_R} = \emptyset$, 
    then
    $\psi_L \odot_\ell \psi_R$ denotes a proof $\langle V, E, \Gamma \rangle$ s.t.
    \begin{align*}
      V &= V_L \cup V_R \cup \{\n \} \\
      E &= E_L \cup E_R \cup
                    \left\{ \raiz{\psi_L} \xrightarrow[\sigma_L]{\ell_L} \n, 
                            \raiz{\psi_R} \xrightarrow[\sigma_R]{\ell_R} \n \right\} \\
     \Gamma &= \left( \clause_L \setminus \left\{ \ell_L \right\} \right) \sigma_L \cup \left( \clause_R
                    \setminus \left\{ \ell_R \right\} \right) \sigma_R 
    \end{align*}
    where $\n$ is a new (resolution) node and $\raiz{\varphi}$ denotes the root node of $\varphi$.
  \item \textbf{Contraction:} If $\psi'$ is a proof $\langle V', E', \clause' \rangle$ and $\sigma$ is a unifier of $\{\ell_1, \ldots \ell_n\}$ with $\{\ell_1, \ldots \ell_n\} \subseteq \clause'$ and $\ell = \ell_k \sigma$ ($1 \leq k \leq n$), 
  then $\con{\psi}{\sigma}{\ell}$ denotes a proof $\langle V, E, \Gamma \rangle$ s.t.
    \begin{align*}
      V &= V' \cup \{\n \} \\
      E &= E' \cup \{ \raiz{\psi'} \xrightarrow[\sigma]{\ell} \n \} \\
     \Gamma &= (\clause' \setminus \{ \ell_1, \ldots \ell_n \} ) \sigma \cup \{ \ell \}
    \end{align*}
    where $\n$ is a new (contraction) node and $\raiz{\varphi}$ denotes the root node of $\varphi$.
  \qed
\end{itemize}
\end{definition}


\newcommand{\Vertices}[1]{V_{#1}}
\newcommand{\Edges}[1]{E_{#1}}
\newcommand{\Conclusion}[1]{\clause_{#1}}

\noindent
If $\psi = \varphi_L \odot_{\ell} \varphi_R$, then $\varphi_L$ and $\varphi_R$ are \emph{direct
subproofs} of $\psi$ and $\psi$ is a \emph{child} of both $\varphi_L$ and $\varphi_R$. The
transitive closure of the direct subproof relation is the \emph{subproof} relation. A subproof which
has no direct subproof is an \emph{axiom} of the proof.
%
$\Vertices{\psi}$, $\Edges{\psi}$ and $\Conclusion{\psi}$
denote, respectively, the nodes, edges and proved clause (conclusion) of $\psi$.


\SetKwFunction{Rec}{delete}
\SetKw{Let}{let}

\begin{algorithm}[bt]
  \KwIn{a proof $\varphi$}
  \KwIn{$D$ a set of subproofs}
  \KwOut{a proof $\varphi'$ obtained by deleting the subproofs in $D$ from $\varphi$}
  \BlankLine

  \newcommand{\fixL}{\ensuremath{\varphi'_L}}
  \newcommand{\fixR}{\ensuremath{\varphi'_R}}

  \uIf{$\varphi \in D$ or $\raiz{\varphi}$ has no premises}{
    \Return{$\varphi$} \;
  }
  \BlankLine

  \Else{
    \Let{$\varphi_L$, $\varphi_R$ and $\ell$} be such that
      $\varphi = \varphi_L \odot_\ell \varphi_R$ \;
    \Let{$\varphi'_L = $ \Rec{$\varphi_L$,$D$}} \;
    \Let{$\varphi'_R = $ \Rec{$\varphi_R$,$D$}} \;
    \BlankLine

    \uIf{$\varphi'_L \in D$}
      { \Return{\fixR} \; }
    \uElseIf{$\varphi'_R \in D$}
      { \Return{\fixL} \; }
    \BlankLine

    \uElseIf{$\dual{\ell} \notin \Conclusion{\fixL}$}
      { \Return{\fixL} \; }
    \uElseIf{$\ell \notin \Conclusion{\fixR}$}
      { \Return{\fixR} \; }
    \BlankLine

    \Else{ \Return{ \fixL~$\odot_\ell$~\fixR} \; }
  }

  \caption[.]{\FuncSty{delete}}
  \label{algo:del}
\end{algorithm}

The deletion algorithm is a minor variant of the \textsc{Reconstruct-Proof} algorithm presented in
\cite{RP11}.
The basic idea is to traverse the proof in a top-down manner, replacing
each subproof having one of its premises marked for deletion (i.e. in $D$) by its other direct
subproof. The special case when both $\varphi'_L$ and $\varphi'_R$ belong to $D$ is treated rather
implicitly and deserves an explanation: in such a case, one might intuitively expect the result
$\varphi'$ to be undefined and arbitrary. Furthermore, to any child of $\varphi$, $\varphi'$ ought
to be seen as if it were in $D$, as if the deletion of $\varphi'_L$ and $\varphi'_R$ propagated to
$\varphi'$ as well. Instead of assigning some arbitrary proof to $\varphi'$ and adding it to $D$,
the algorithm arbitrarily returns (in line 8) $\varphi'_R$ (which is already in $D$) as the result
$\varphi'$. In this way, the propagation of deletion is done automatically and implicitly. For
instance, the following hold:
\begin{align}
  \dn{\varphi_1 \odot_\ell \varphi_2}{\varphi_1, \varphi_2} &= \varphi_2 \label{eq:exampledel1} \\
\dn{\varphi_1 \odot_\ell \varphi_2 \odot_{\ell'} \varphi_3}{\varphi_1, \varphi_2} &=
  \dn{\varphi_3}{\varphi_1, \varphi_2} \label{eq:exampledel2}
\end{align}
A side-effect of this clever implicit propagation of deletion is that the actual result of deletion
is only meaningful if it is not in $D$. In the example (\ref{eq:exampledel1}), as $\dn{\varphi_1
\odot_\ell \varphi_2}{\varphi_1, \varphi_2} \in \{\varphi_1, \varphi_2\} $, the actual resulting
proof is meaningless. Only the information that it is a deleted subproof is relevant, as it suffices
to obtain meaningful results as shown in (\ref{eq:exampledel2}).

\begin{proposition} \label{prop:del_assoc}
For any proof $\psi$ and any sets $A$ and $B$ of $\psi$'s subproofs,
either $\dn{\psi}{A \cup B}  \in A \cup B$
and    $\dn{\dn{\psi}{A}}{B} \in A \cup B$,
or     $\dn{\psi}{A \cup B} = \dn{\dn{\psi}{A}}{B}$.
\end{proposition}




\newcommand{\pedge}[3]{\ensuremath{\raiz{#1} \xrightarrow{#2} \raiz{#3}}}




\section{LowerUnits} \label{sec:LU}

When a subproof $\varphi$ has more than one child in a proof $\psi$, it may be possible to \emph{factor} all
the corresponding resolutions: a new proof is
constructed by removing $\varphi$ from $\psi$ and reintroducing it later. The resulting proof is smaller because $\varphi$ participates in a single resolution inference in it (i.e. it has a single child), while in the original proof it participates in as many resolution inferences as the number of children it had. Such a factorization is called \emph{lowering} of $\varphi$, because its delayed reintroduction makes $\varphi$ appear at the bottom of the resulting proof. 

Formally, a subproof $\varphi$ in a proof $\psi$ can be lowered if there exists a proof
$\psi'$ and a literal $\ell$ such that $\psi' = \dn{\psi}{\varphi} \odot_{\ell} \varphi$ and
$\Conclusion{\psi'} \subseteq \Conclusion{\psi}$. It has been noted in \cite{LURPI} that $\varphi$ can always be lowered if it is a \emph{unit}: its conclusion clause has only one literal. This led to the invention of the {\LowerUnits} algorithm, which lowers every unit with more than one child, taking care to reintroduce units in
an order corresponding to the subproof relation: if a unit $\varphi_2$ is a subproof of a unit
$\varphi_1$ then $\varphi_2$ has to be reintroduced later than (i.e. below) $\varphi_1$.

A possible presentation of {\LowerUnits} is shown in Algorithm \ref{algo:LU}. Units are collected
during a first traversal. As this traversal is bottom-up, units are stored in a queue. The traversal
could have been top-down and units stored in a stack. Units are effectively deleted during a second,
top-down traversal. The last for-loop performs the reintroduction of units.

\begin{algorithm}[bt]
  \KwIn {a proof $\psi$}
  \KwOut{a compressed proof $\psi'$}
  \BlankLine

  \SetKwData{Units}{Units}
  \Units $\leftarrow \varnothing$ \;
  \BlankLine

  \For{every subproof $\varphi$ in a bottom-up traversal}{
    \If{$\varphi$ is a unit and has more than one child}{Enqueue $\varphi$ in \Units \; }
  }
  \BlankLine

  $\psi' \leftarrow $ \Rec{$\psi$,$\Units$} \;
  \BlankLine

  \For{every unit $\varphi$ in \Units}{
    \Let{$\{\ell\} = \Conclusion{\varphi}$} \;
    \lIf{$\dual{\ell} \in \Conclusion{\psi'}$}{
    $\psi' \leftarrow \psi' \odot_\ell \varphi$ \;}
  }

  \caption{\LowerUnits}
  \label{algo:LU}
\end{algorithm}



\section{First-Order LowerUnits} \label{sec:FOLU}

{\LowerUnits} does not lower every lowerable subproof. In particular, it does not take into
account the already lowered subproofs. For instance, if a unit $\varphi_1$ proving $\{a\}$ has
already been lowered, a subproof $\varphi_2$ with conclusion $\{\neg a, b\}$ may be lowered as well and
reintroduced above $\varphi_1$. The posterior reintroduction of $\varphi_1$ will resolve away $\neg a$ and guarantee that it does not occur in the resulting proof's conclusion. But care must also be taken not to lower $\varphi_2$ if $\neg a$ is a valent literal of
$\varphi_2$, otherwise $a$ will undesirably occur in the resulting proof's conclusion.

\begin{definition}[Univalent subproof]
A subproof $\varphi$ in a proof $\psi$ is \emph{univalent} w.r.t. a set $\Delta$ of literals iff
$\varphi$ has exactly one valent literal $\ell$ in $\psi$, $\ell \notin \Delta$ and
$\Conclusion{\varphi} \subseteq \Delta \cup \left\{ \ell \right\}$. $\ell$ is called the \emph{univalent
literal} of $\varphi$ in $\psi$ w.r.t.  $\Delta$.
\end{definition}

The principle of {\LowerUnivalents} is to lower all univalent subproofs. Having only one valent literal makes them behave essentially like units w.r.t. the technique of lowering. $\Delta$ is
initialized to the empty set. Then the complements of the univalent literals are incrementally added to
$\Delta$. Proposition \ref{prop:LUniv} ensures that the conclusion of the resulting proof
subsumes the conclusion of the original one.

\begin{proposition} \label{prop:LUniv}
Given a proof $\psi$, if 
%for an integer $n$
there is a sequence $U = (\varphi_1 \ldots \varphi_n)$
of $\psi$'s subproofs and a sequence $(\ell_1 \ldots \ell_n)$ of literals such that $\forall i \in
[1 \ldots n]$, $\ell_i$ is the univalent literal of $\varphi_i$ w.r.t. $\Delta_{i-1} =
\{\dual{\ell_1} \ldots \dual{\ell_{i-1}}\}$, then the conclusion of $$ \psi' = \dn{\psi}{U}
\odot_{\ell_n} \varphi_n \ldots \odot_{\ell_1} \varphi_1 $$ subsumes the conclusion of $\psi$.
\end{proposition}

\begin{proof}
The proposition is proven by induction on $n$, along with the fact that $\dn{\psi}{U} \notin U$.
For $n = 0$, $U = \varnothing$ and the properties trivially hold. Suppose a subproof
$\varphi_{n+1}$ of $\psi$ is univalent w.r.t. $\Delta_n$, with univalent literal $\ell_{n+1}$.
Because $\ell_{n+1} \notin \Delta_n$, there exists a subproof of $\dn{\psi}{U}$ with conclusion
containing $\dual{\ell_{n+1}}$, and therefore $\dn{\dn{\psi}{U}}{\varphi_{n+1}} \notin U \cup
\{\varphi_{n+1}\}$.  Let $\Gamma$ be the conclusion of $\dn{\psi}{U}$. The conclusion of $ \psi' =
\dn{\psi}{U \cup \{\varphi_{n+1}\}} = \dn{\dn{\psi}{U}}{\varphi_{n+1}} $ is included in $\Gamma \cup
\{\dual{\ell_{n+1}}\}$. The conclusion of $\psi' \odot_{\ell_{n+1}} \varphi_{n+1}$ is included in
$\Gamma \cup \Delta_n$. As $\Gamma \subseteq \Conclusion{\psi} \cup \Delta_n$, the conclusion of
$\psi' \odot_{\ell_{n+1}} \varphi_{n+1} \ldots \odot_{\ell_1} \varphi_1$ is included in
$\Conclusion{\psi}$. \qed
\end{proof}

For this principle to lead to proof compression, it is important to take care
of the mutual inclusion of univalent subproofs.
%not only of the order in which subproofs are collected for lowering but also of deleting all
%already collected univalent subproofs from the next subproof $\psi_i$ before reintroducing it.
Suppose, for instance, that $\varphi_i, \varphi_j, \varphi_k \in U$, $i < j < k$, $\varphi_j$ is a
subproof of $\varphi_i$ but not a subproof of $\dn{\psi}{\varphi_i}$, and $\dual{\ell_j} \in
\Conclusion{\varphi_k}$.  In this case, $\varphi_j$ will have one more child in
$$
\dn{\psi}{U} \odot_{\ell_n} \varphi_n \ldots \odot_{\ell_k} \varphi_k \ldots \odot_{\ell_j} \varphi_j \ldots \odot_{\ell_i} \varphi_i \ldots \odot_{\ell_1} \varphi_1
$$
than in the original proof $\psi$. The additional child is created when $\varphi_j$ is reintroduced.
All the other children are reintroduced with the reintroduction of $\varphi_i$, because
$\varphi_j$ was not deleted from $\varphi_i$.

To solve this issue, {\LowerUnivalents} traverses the proof in a top-down manner and simultaneously
deletes already collected univalent subproofs, as sketched in Algorithm \ref{algo:LUniv}.  


\SetKwData{Univ}{Univalents}
\begin{algorithm}[bt]
  \KwIn {a proof $\psi$}
  \KwOut{a compressed proof $\psi'$}
  \BlankLine

  \SetKw{Push}{push}
  \SetKw{Pop} {pop}

  \Univ $\leftarrow \varnothing$ \;
  $\Delta \leftarrow \varnothing$ \;
  \BlankLine

  \For{every subproof $\varphi$, in a top-down traversal \label{line:LUniv:step1begin} }{
    $\psi' \leftarrow$ \Rec{$\varphi$,\Univ} \label{line:LUniv:delete} \;
    \If{$\psi'$ is univalent w.r.t. $\Delta$ \label{line:LUniv:lunivtest} }{
      \Let{$\ell$} be the univalent literal \;
      \Push $\dual{\ell}$ onto $\Delta$ \label{line:LUniv:pushDelta} \;
      \Push $\psi'$     onto \Univ \label{line:LUniv:step1end} \;
    }
  }
  \BlankLine

  \tcp{At this point, $\psi' = \dn{\psi}{\Univ}$}
  \While{\Univ $\neq \varnothing$}{ \label{line:LUniv:reintroducebegin}
    $\varphi \leftarrow$ \Pop from \Univ \;
    $\ell \leftarrow$ \Pop from $\Delta$ \;
    \lIf{$\ell \in \Conclusion{\psi'}$ \label{line:LUniv:testreintroduce} }{
    $\psi' \leftarrow \varphi \odot_\ell \psi'$ \;}
  }

  \caption{Simplified \LowerUnivalents}
  \label{algo:LUniv}
\end{algorithm}


Figure \ref{fig:exluniv} shows an example proof and the result of compressing it with \LowerUnivalents. The top-down traversal starts with the leaves (axioms) and only visits a child when all its parents have already been visited. Assuming the unit with conclusion $\{a\}$ is the first visited leaf, it passes the univalent test in line \ref{line:LUniv:lunivtest}, is marked for lowering (line \ref{line:LUniv:step1end}) and the complement of its univalent literal is pushed onto $\Delta$ (line \ref{line:LUniv:pushDelta}). When the subproof with
conclusion $\{\dual{a},b\}$ is considered, $\Delta = \{\dual{a}\}$. As this subproof has only one
valent literal $b \notin \Delta$ and $\{\dual{a},b\} \subseteq \Delta \cup \{b\}$, it is
marked for lowering as well. At this point, $\Delta = \{\dual{a}, \dual{b}\}$, \texttt{Univalents} contains the two subproofs marked for lowering and $\psi'$ is the subproof with conclusion $\{\dual{a}, \dual{b}\}$ shown in Subfig. (b) (i.e. the result of deleting the two marked subproofs from the original proof in Subfig. (a)). No other subproof is univalent; no other subproof is marked for lowering. The final compressed proof (Subfig. (b)) is obtained by reintroducing the two univalent subproofs that had been marked (lines \ref{line:LUniv:reintroducebegin} -- \ref{line:LUniv:testreintroduce}). It has one resolution less than the original. This is so because the subproof with conclusion $\{\dual{a},b\}$ had been used (resolved) twice in the original proof, but lowering delays its use to a point where a single use is sufficient.

\begin{figure}[htb]
  \centering
  \subfloat[Original proof]{
    \centering
    \begin{tikzpicture}

      \rootnode;
      \withchildren{root} {r0}{\dual{a}}  {unit}{a};
      \withchildren{r0}   {r1}{\dual{a},c} {r2}{\dual{a},\dual{c}};
      \withchildren{r1}   {a0}{\dual{b},c} {low}{\dual{a},b};

      \proofnode[above right of=r2] {a1} {\dual{a},\dual{b},\dual{c}};
      \drawchildren {r2} {low} {a1};

    \end{tikzpicture}
  } \qquad
  \centering
  \subfloat[Compressed proof]{
    \centering
    \begin{tikzpicture}

      \rootnode;
      \withchildren{root} {r0}{\dual{a}}          {unit}{a};
      \withchildren{r0}   {r1}{\dual{a},\dual{b}} {low}{\dual{a},b};
      \withchildren{r1}   {a0}{\dual{b},c}        {a1}{\dual{a},\dual{b},\dual{c}};

    \end{tikzpicture}
  }
\caption{Example of proof crompression by \LowerUnivalents} 
\label{fig:exluniv}
\end{figure}


% Discussion of optimizations follow

Although the
call to \FuncSty{delete} inside the first loop (line \ref{line:LUniv:step1begin} to
\ref{line:LUniv:step1end}) suggests quadratic time complexity, this loop (line
\ref{line:LUniv:step1begin} to \ref{line:LUniv:step1end}) can be (and has been) actually implemented
as a recursive function extending a recursive implementation of \FuncSty{delete}. With such an
implementation, {\LowerUnivalents} has a time complexity linear w.r.t. the size of the proof, assuming the
univalent test (at line \ref{line:LUniv:lunivtest}) is performed in constant bounded time. 


Determining whether a literal is valent is expensive. But thanks to Proposition \ref{prop:valentactive},
subproofs with one active literal which is not in $\Conclusion{\psi}$ can be considered instead
of subproofs with one valent literal.  If the active literal is not valent, the corresponding
subproof will simply not be reintroduced later (i.e. the condition in line 28 of Algorithm \ref{algo:fullLUniv} will fail).

While verifying if a subproof could be univalent, some edges might be deleted. If a
subproof $\varphi_i$ has already been collected as univalent subproof with univalent literal
$\ell_i$ and the subproof $\varphi'$ being considered now has $\ell_i$ as active literal, the
corresponding incoming edges can be removed. Even if $\ell_i$ is valent for $\varphi'$, only
$\dual{\ell_i}$ would be introduced, and it would be resolved away when reintroducing
$\varphi_i$. The \FuncSty{delete} operation can be easily modified to remove both nodes and edges.

Algorithm \ref{algo:fullLUniv} sums up the previous remarks for an efficient implementation of
{\LowerUnivalents}. As noticed above, sometimes this algorithm may consider a subproof as univalent when it
is actually not. But as care is taken when reintroducing subproofs (at line \ref{line:full:testreintroduce}),
the resulting conclusion still subsumes the original.  The test that $\ell \in \Conclusion{\varphi}$
at line \ref{line:full:testactive} is mandatory since $\ell$ might have been deleted from
$\Conclusion{\varphi}$ by the deletion of previously collected subproofs.

\begin{algorithm}[pbt]
  \SetAlgoVlined
  \SetAlgoShortEnd

  \KwData {a proof $\psi$, compressed in place}
  \KwIn {a set $D_V$ of subproofs to delete}
  \KwIn {a set $D_E$ of edges to delete}
%  \KwOut{the proof $\psi$ compressed in place}
  \BlankLine

  \SetKw{Push}{push}
  \SetKw{Pop} {pop}
  \SetKw{Add} {add}
  \SetKw{Rep} {replace}

  \SetKwData{Activ}{ActiveLiterals}

  \Univ $\leftarrow \varnothing$ \;
  $\Delta \leftarrow \varnothing$ \;
  \BlankLine

  \For{every subproof $\varphi$, in a top-down traversal of $\psi$ }{

    \tcp{The deletion part.}
    \If{$\varphi$ is not an axiom}{
      \Let{$\varphi = \varphi_L \odot_\ell \varphi_R$} \;
      \uIf{ $\varphi_L \in D_V$ or $\pedge{\varphi}{\dual{\ell}}{\varphi_L} \in D_E$ }{
        \uIf{ $\pedge{\varphi}{\ell}{\varphi_R} \in D_E$ }{
          \Add $\varphi$ to $D_V$ \;
        }
        \Else{
          \Rep $\varphi$ by $\varphi_R$ \;
        }
      }
      \ElseIf{ $\varphi_R \in D_V$ or $\pedge{\varphi}{\dual{\ell}}{\varphi_R} \in D_E$ }{
        \uIf{ $\pedge{\varphi}{\ell}{\varphi_L} \in D_E$ }{
          \Add $\varphi$ to $D_V$ \;
        }
        \Else{
          \Rep $\varphi$ by $\varphi_L$ \;
        }
      }
    }
    \BlankLine
    
    \tcp{Test whether $\varphi$ is univalent.}
    \Activ $\leftarrow \varnothing$ \;
    \For{each incoming edge $e = \n \xrightarrow{\ell} \raiz{\varphi}$, $e \notin D_E$ }{
      \uIf{$\dual{\ell} \in \Delta$}{
        \Add $e$ to $D_E$ \;
      }
      \ElseIf{$\ell \notin \Delta$, $\ell \in \Conclusion{\varphi}$ \label{line:full:testactive}
              and $\ell \notin \Conclusion{\psi}$ }{
        \Add $\ell$ to \Activ \;
      }
    }

%    \BlankLine
    \If{\Activ $= \{\ell\}$ and $\Conclusion{\varphi} \subseteq \Delta \cup \{\ell\}$ }{
      \Push $\dual{\ell}$ onto $\Delta$ \;
      \Push $\varphi$     onto \Univ  \;
    }
  }
  \BlankLine

  \tcp{Reintroduce lowered subproofs.}
  \While{\Univ $\neq \varnothing$}{
    $\varphi \leftarrow$ \Pop from \Univ \;
    $\ell \leftarrow$ \Pop from $\Delta$ \;
    \If{$\ell \in \Conclusion{\psi}$ \label{line:full:testreintroduce}  }{
      \Rep $\psi$ by $\varphi \odot_\ell \psi$ \;}
  }

  \caption{Optimized {\LowerUnivalents} as an enhanced \texttt{delete}}
  \label{algo:fullLUniv}
\end{algorithm}


Every node in a proof $\langle V, E, \Gamma \rangle$ has exactly two outgoing edges unless it is the
root of an axiom. Hence the number of axioms is $|V| - \frac{1}{2}\,|E|$ and because there is at
least one axiom, the average number of active literals per node is strictly less than two.
Therefore, if {\LowerUnivalents} is implemented as an improved recursive \FuncSty{delete}, its time
complexity remains linear, assuming membership of literals to the set $\Delta$ is computed in constant
time.

\begin{proposition} \label{prop:compression}
Given a proof $\psi$,
{\LowerUnits\unskip\FuncSty{(}$\psi$\FuncSty{)}}
has at least as many nodes as 
{\LowerUnivalents\unskip\FuncSty{(}$\psi$\FuncSty{)}}
if there are no two units in $\psi$ with the same conclusion.
\end{proposition}

\begin{proof}
A unit $\varphi$ has exactly one active literal $\ell$. Therefore $\varphi$ is collected by
{\LowerUnivalents} unless $\dual{\ell} \in \Delta$ or $\ell \in \Delta$. If $\dual{\ell} \in \Delta$
all the incoming edges to $\raiz{\varphi}$ are deleted. If $\ell \in \Delta$, every edge
$\n \xrightarrow{\dual{\ell}} \n'$ where $\n$ is on a path from $\raiz{\psi}$ to $\raiz{\varphi}$
is deleted.
%coming from a descendent of $\raiz{\varphi}$ and labeled by $\dual{\ell} are deleted.
In particular, for every edge $\n \xrightarrow{\ell} \raiz{\varphi}$ the edge $\n
\xrightarrow{\dual{\ell}} \n'$ is deleted.  Moreover, as $\ell$ is the only literal of $\varphi$'s
conclusion, $\varphi$ is propagated down the proof until the univalent subproof with valent literal
$\dual{\ell}$ is reintroduced. \qed
\end{proof}

In the case where there are at least two units with the same conclusion in $\psi$, the
compressed proof depends on the order in which the units are collected. For both algorithms, only one of these units appears in the compressed proof.



\section{Remarks about Combining {\LowerUnivalents} with {\RPI}} \label{sec:LUnivRPI}

\begin{definition}[Regular proof \cite{Tseitin}]
A proof $\psi$ is \emph{regular} iff on every path from its root to any of its axioms, each literal
labels at most one edge. Otherwise, $\psi$ is \emph{irregular}.
\end{definition}

Any irregular proof can be converted into a regular proof having the same axioms and the same
conclusion. But it has been proved \cite{Goerdt} that such a total regularization might result in a
proof exponentially bigger than the original. 

Nevertheless, \emph{partial} regularization algorithms, such as {\RecyclePivots} \cite{RP08} and {\RecyclePivotsIntersection} ({\RPI}) \cite{LURPI}, carefully avoid the worst case of total regularization and do efficiently compress proofs.  For
any subproof $\varphi$ of a proof $\psi$, {\RPI} removes the edge $\raiz{\varphi} \xrightarrow{\ell}
\n$ if $\ell$ is a safe literal for $\varphi$.

\begin{definition}[Safe literal]
A literal $\ell$ is \emph{safe} for a subproof $\varphi$ in a proof $\psi$ iff $\ell$ labels at
least one edge on every path from $\raiz{\psi}$ to $\raiz{\varphi}$.
\end{definition}

{\RPI} performs two traversals. During the first one, safe literals are collected and edges are
marked for deletion. The second traversal is the effective deletion similar to the
$\FuncSty{delete}$ algorithm.

Both sequential compositions of {\LowerUnits} with {\RPI} have been shown to achieve good
compression ratio \cite{LURPI}. However, the best combination order ({\LowerUnits} after
{\RPI} (\texttt{LU.RPI}) or {\RPI} after {\LowerUnits} (\texttt{RPI.LU})) depends on the input proof. A reasonable solution is to perform both combinations and then to choose the smallest
compressed proof, but sequential composition is time consuming. To speed up DAG traversal, it is useful to topologically sort the nodes of the graph first. But in case of sequential composition this costly operation has to be done twice. Moreover, some traversals, like deletion, are identical in both algorithms and might be shared. Whereas implementing a non-sequential combination of {\RPI} after {\LowerUnits} is not difficult, a non-sequential combination of {\LowerUnits} after {\RPI} would be complicated. 
The difficulty is that {\RPI} could create some new units which would be visible only after the deletion
phase.  A solution could be to test for units during deletion. But if units are
effectively lowered during this deletion, their deletion would cause some units to become non-units.
And postponing deletions of units until a second deletion traversal would prevent the sharing of
this traversal and would cause one more topological sorting to be performed, because the deletion phase significantly transforms the structure of the DAG.

Apart from having an improved compression ratio, another advantage of {\LowerUnivalents} over
{\LowerUnits} is that {\LowerUnivalents} can be implemented as an enhanced \FuncSty{delete}
operation. With such an implementation, a simple non-sequential combination of {\LowerUnivalents}
after {\RPI} can be implemented just by replacing the second traversal of {\RPI} by
{\LowerUnivalents}. After the first traversal of {\RPI}, as all edges labeled by a safe literal have been marked for deletion, the remaining active literals are all valent, because for every edge $\pedge{\varphi}{\ell}{\varphi'}$, $\ell$ is either a safe literal
of $\varphi$ or a valent literal of $\varphi'$.  Therefore, in the second traversal of the non-sequential combination (deletion enhanced by {\LowerUnivalents}), all univalent subproofs are lowered.



\section{Experiments} \label{sec:exp}

ToDo by Jan

% {\LowerUnivalents} and {\LUnivRPI} have been implemented in the functional programming
% language Scala\footnote{\url{http://www.scala-lang.org/}} as part of the \skeptik
% library\footnote{\url{https://github.com/Paradoxika/Skeptik}}. {\LowerUnivalents} has been implemented as a
% recursive \FuncSty{delete} improvement.

% The algorithms have been applied to 5\,059 proofs produced by the SMT-solver
% {\veriT}\footnote{\url{http://www.verit-solver.org/}} on unsatisfiable benchmarks from the
% SMT-Lib\footnote{\url{http://www.smtlib.org/}}.  The details on the number of proofs per SMT category
% are shown in Table \ref{tab:benchmarks}.  The proofs were translated into pure resolution proofs by
% considering every non-resolution inference as an axiom.

% \begin{table}[tb]
%   \caption{Number of proofs per benchmark category}
%   \label{tab:benchmarks}
%   \centering
%   \begin{tabular}{lr}
%     \toprule
%     Benchmark~ &  Number \\
%     Category       & ~of Proofs \\
%     \midrule
%     QF\_UF      & 3907 \\
%     QF\_IDL     &  475 \\
%     QF\_LIA     &  385 \\
%     QF\_UFIDL   &  156 \\
%     QF\_UFLIA   &  106 \\
%     QF\_RDL     &   30 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% The experiment compared the following algorithms:
% \begin{description}
%   \item[LU:] the {\LowerUnits} algorithm from \cite{LURPI};
%   \item[LUniv:] the {\LowerUnivalents} algorithm;
%   \item[RPILU:] a non-sequential combination of {\RPI} after {\LowerUnits};
%   \item[RPILUniv:] a non-sequential combination of {\RPI} after {\LowerUnivalents};
%   \item[LU.RPI:] the sequential composition of {\LowerUnits} after {\RPI};
%   \item[LUnivRPI:] the non-sequential combination of {\LowerUnivalents} after {\RPI} as described in Sect. \ref{sec:LUnivRPI};
%   \item[RPI:] the {\RecyclePivotsIntersection} from \cite{LURPI};
%   \item[Split:] Cotton's \texttt{Split} algorithm (\cite{CottonSplit});
%   \item[RedRec:] the {\ReduceReconstruct} algorithm from \cite{RedRec};
%   \item[Best RPILU/LU.RPI:] which performs both \texttt{RPILU} and \texttt{LU.RPI} and chooses the smallest resulting compressed proof;
%   \item[Best RPILU/LUnivRPI:] which performs \texttt{RPILU} and \texttt{LUnivRPI} and chooses the smallest resulting
%     compressed proof.
% \end{description}

% For each of these algorithms, the time needed to compress the proof along with the number of nodes
% and the number of axioms (i.e. \emph{unsat core} size) have been measured. Raw data of the
% experiment can be downloaded from the web\footnote{\url{http://www.matabio.net/skeptik/LUniv/experiments/}}.

% The experiments were executed on the Vienna Scientific Cluster\footnote{\url{http://vsc.ac.at/}}
% VSC\nobreakdash-2. Each algorithm was executed in a single core and had up to 16 GB of memory available. This amount of memory has been useful to compress the biggest proofs (with more than $10^6$ nodes).


% The overall results of the experiments are shown in Table \ref{tab:average}. The compression ratios
% in the second column are computed according to formula (\ref{eq:compression}), in which $\psi$
% ranges over all the proofs in the benchmark and $\psi'$ ranges over the corresponding compressed
% proofs.
% \begin{equation} \label{eq:compression}
%   1 - \frac{ \sum {|\Vertices{\psi'}|} }{ \sum {|\Vertices{\psi}|} }
% \end{equation}
% The unsat core compression ratios are computed in the same way, but using the number of axioms instead of
% the number of nodes. The speeds on the fourth column are computed according to formula
% (\ref{eq:speed}) in which $d_{\psi}$ is the duration in milliseconds of $\psi$'s compression by a
% given algorithm.
% \begin{equation} \label{eq:speed}
%   \frac{ \sum {|\Vertices{\psi}|} }{ \sum {d_{\psi}} }
% \end{equation}

% For the \texttt{Split} and \texttt{RedRec} algorithms, which must be repeated, a timeout has
% been fixed so that the speed is about 3 nodes per millisecond. 


% \begin{table}[tb]
%   \caption{Total compression ratios}
%   \label{tab:average}
%   \centering
%   \begin{tabular}{lrrr}
%     \toprule
%     \multirow{2}{*}{Algorithm} & \multirow{2}{*}{Compression} & Unsat Core    & \phantom{.........}\multirow{2}{*}{Speed} \\
%                                              &                                               & \phantom{...}Compression &        \\
%     \midrule
%     LU                &  7.5 \% &  0.0 \% & 22.4 n/ms \\
%     LUniv             &  8.0 \% &  0.8 \% & 20.4 n/ms \\
%     RPILU             & 22.0 \% &  3.6 \% &  7.4 n/ms \\
%     RPILUniv          & 22.1 \% &  3.6 \% &  6.5 n/ms \\
%     LU.RPI            & 21.7 \% &  3.1 \% & 15.1 n/ms \\
%     LUnivRPI          & 22.0 \% &  3.6 \% & 17.8 n/ms \\
%     RPI               & 17.8 \% &  3.1 \% & 31.3 n/ms \\
%     Split             & 21.0 \% &  0.8 \% &  2.9 n/ms \\
%     RedRec            & 26.4 \% &  0.4 \% &  2.9 n/ms \\
%     Best RPILU/LU.RPI       & 22.0 \% &  3.7 \% &  5.0 n/ms \\
%     Best RPILU/LUnivRPI & 22.2 \% &  3.7 \% &  5.2 n/ms \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \newcommand{\va}[1]{\ensuremath{v_{\text{#1}}}}

% Figure \ref{fig:LU} shows the comparison of {\LowerUnits} with {\LowerUnivalents}. Subfigures (a) and (b) are scatter plots where each dot represents a single benchmark proof. 
% Subfigure (c) is a histogram showing, in the vertical axis, the proportion of proofs having \emph{(normalized) compression ratio difference} within the intervals showed in the horizontal axis. This difference is computed using formula (\ref{eq:histogram}) with \va{LU}
% and \va{LUniv} being the compression ratios obtained respectively by {\LowerUnits} and
% {\LowerUnivalents}.
% \begin{equation} \label{eq:histogram}
%   \frac { \va{LU} - \va{LUniv} }{ \frac{\va{LU} + \va{LUniv}}{2} }
% \end{equation}
% The number of proofs for which $\va{LU} = \va{LUniv}$ is not displayed in the histogram.
% The \emph{(normalized) duration differences} in subfigure (d) are computed using the same formula~(\ref{eq:histogram}) but
% with \va{LU} and \va{LUniv} being the time taken to compress the proof by {\LowerUnits} and
% {\LowerUnivalents} respectively.

% \input{LU_charts}

% As expected, {\LowerUnivalents} always compresses more than {\LowerUnits} (subfigure (a)) at the expense of a longer
% computation (subfigure (d)). And even if the compression gain is low on average (as noticeable in Table \ref{tab:average}), subfigure (a) shows that {\LowerUnivalents} compresses some proofs significantly more than {\LowerUnits}.

% It has to be noticed that \veriT already does its best to produce compact proofs. In particular,
% a forward subsumption algorithm is applied, which results in proofs not having two different subproofs
% with the same conclusion. This results in {\LowerUnits} being unable to reduce unsat core.
% But as {\LowerUnivalents} lowers non-unit subproofs and performs some partial regularization, it
% achieves some unsat core reduction, as noticeable in subfigure (b).

% The comparison of the sequential \texttt{LU.RPI} with the non-sequential {\LUnivRPI} shown in Fig.
% \ref{fig:LUnivRPI} outlines the ability of {\LowerUnivalents} to be efficiently combined with other
% algorithms. Not only compression ratios are improved but {\LUnivRPI} is faster than the sequential
% composition for more than 80 \% of the proofs.



% \input{LURPI_charts}



\section{Conclusions and Future Work}

{\LowerUnivalents}, the algorithm presented here, has been shown in the previous section to compress
more than {\LowerUnits}. This is so because, as demonstrated in Proposition \ref{prop:compression}, the
set of subproofs it lowers is always a superset of the set of subproofs lowered by {\LowerUnits}. It might
be possible to lower even more subproofs by finding a characterization of (efficiently) lowerable subproofs
broader than that of univalent subproofs considered here. This direction for future work promises to be challenging, though, as evidenced by the non-triviality of the optimizations discussed in Section \ref{sec:LUniv} for obtaining a linear-time implementation of {\LowerUnivalents}.



As discussed in Section \ref{sec:LUnivRPI}, the proposed algorithm can be embedded in the deletion traversal of other algorithms.  As
an example, it has been shown that the combination of {\LowerUnivalents} with {\RPI}, compared to
the sequential composition of {\LowerUnits} after {\RPI}, results in a better compression ratio with
only a small processing time overhead (Figure \ref{fig:LUnivRPI}). Other compression algorithms that also have a subproof
deletion or reconstruction phase (e.g. \ReduceReconstruct) could probably benefit from being
combined with {\LowerUnivalents} as well.

%\vspace{-10pt}
%\paragraph{Acknowledgments:}



\bibliographystyle{splncs}
\bibliography{biblio}


\end{document}

% vim: tw=100
