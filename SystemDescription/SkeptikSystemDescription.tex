\documentclass{llncs}

\usepackage{xcolor}
\usepackage{enumitem,amsmath,amssymb}
\usepackage{breakurl}    % used for \url and \burl
\usepackage[linesnumbered,boxed,noline,noend]{algorithm2e}
\def\defaultHypSeparation{\hskip.1in}

\usepackage{tikz}
\usepackage{subfig}
\usepackage{array,booktabs,multirow}
\usepackage{placeins}

\usepackage{graphicx}
\usepackage{wrapfig}

\usepackage{logictools}
\usepackage{prooftheory}
\usepackage{comment}
\usepackage{mathenvironments}
\usepackage{drawproof}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}

\title{ 
Skeptik [System Description]
}

\author{
  Joseph Boudou\inst{1}
  %\thanks{Supported by the Google Summer of Code 2012 program.}
  \and 
  Andreas Fellner\inst{2,3}
  %\thanks{Supported by the Google Summer of Code 2013 program.}
  \and 
  Bruno Woltzenlogel Paleo\inst{3}
  \thanks{Funded by Google Summer of Code 2012 and 2013 and FWF project P24300.}
}

\authorrunning{J.\~Boudou \and A. Fellner \and B.\~Woltzenlogel Paleo}

\institute{
  IRIT, Universit\'e de Toulouse, France \\
  \email{joseph.boudou@irit.fr}
  \and 
  Free University of Bolzano, Italy \\
  \email{fellner.a@gmail.com}
  \and 
  Vienna University of Technology, Austria \\
  \email{bruno@logic.at}
}




\begin{document}

\maketitle


\begin{abstract}
This paper introduces \skeptik: a system for checking, compressing and improving proofs obtained by automated reasoning tools, especially sat- and smt-solvers.
\end{abstract}

\setcounter{footnote}{0}


\section{Introduction}

There are various reasons why it is desirable 
for automated reasoning
tools to output not only a \emph{yes} or \emph{no} answer to a problem 
but also \emph{proofs/refutations} or \emph{(counter)models}. 
%
Firstly, state-of-the-art tools are complex and heavily optimized. Their code is often long, hard to understand and difficult to automatically verify. Consequently, the \emph{yes/no} answers cannot be fully trusted, unless they are accompanied by proofs or (counter)models that serve as independently checkable certificates of 
their correctness. 

Furthermore, for most applications, a \emph{yes/no} answer is inherently insufficient. We often already known in advance whether a problem is expected to be satisfiable or unsatisfiable, and we want more than just a confirmation of this expectation. For satisfiable formulas, the desired information is encoded in the model; while for valid formulas, it is contained in the proof. In case the expectation was wrong, refutations and countermodels can be very helpful to explain issues in the encoding of the problem, in order to correct and refine it. 
% Proofs and refutations can been used, for example, to extract unsatisfiable cores \cite{ToDo}, interpolants \cite{ToDo}, Herbrand disjunctions \cite{ToDo}, linguistic implicatures \cite{ToDo} \ldots

Although current automated deduction tools are very efficient at finding proofs, they do not necessarily find the best proofs. {\skeptik} efficiently finds and eliminates redundancies in proofs, thus compressing and improving them according to various metrics (\url{http://github.com/Paradoxika/Skeptik/}).

\paragraph{Related Work:} \texttt{CERes} (\url{http://www.logic.at/ceres}) \cite{ToDo} is another proof transformation system, especialized in cut-elimination for classical first-order sequent calculus. It was replaced by \texttt{GAPT} (\url{http://code.google.com/p/gapt/}) \cite{ToDo}, extended with cut-introduction techniques \cite{ToDo}. \texttt{MINLOG} (\url{http://www.mathematik.uni-muenchen.de/~logik/minlog/}) \cite{ToDo} extracts functional programs from proofs, employing a refined A-translation for functional proofs.


\newcommand{\com}[1]{[\texttt{#1}]}

\section{Installation}

{\skeptik} is implemented in Scala and runs on the java virtual machine (JVM). Therefore, Java (\url{https://www.java.com/}) must be installed. The easiest way to download {\skeptik} is via \texttt{git} (\url{http://git-scm.com/}), by executing \com{git clone git@github.com:Paradoxika/Skeptik.git} in the folder where {\skeptik} should be downloaded. It is helpful to install \texttt{SBT} (\url{http://www.scala-sbt.org/}), a build tool that automatically downloads all compilers and libraries on which {\skeptik} depends. To compile, build and package {\skeptik}, run \com{sbt one-jar} in {\skeptik}'s home folder. This generates a jar file that can be executed like any other Java jar file.
%
\texttt{SBT} and Scala programs may need a lot of memory for compilation and execution. If out-of-memory problems occur, the JVM's maximum available memory can be increased by executing \com{export JAVA\_TOOL\_OPTIONS=``-Xmx1024m''} in the terminal.


\section{Usage}

The command \com{java -jar skeptik.jar --help} displays a help message explaining how to use {\skeptik}. For example, to compress the proof ``eq\_diamond9.smt2'' using the algorithm \texttt{RPI} and write the compressed proof using the `smt2' proof format, the following command should be executed: \com{java -jar skeptik.jar -a RPI -f smt2 examples/proofs/VeriT/eq\_diamond9.smt2}. {\skeptik} can be called with an arbitrary number of algorithms and proofs. The following command would compress the proofs ``p1.smt2'' and ``p2.smt2'' with two algorithms each (\texttt{RP} and a sequential composition of \texttt{DAGify}, \texttt{RPI} and \texttt{LU}): \\
\com{java -jar skeptik.jar -a RP -a (DAGify*RPI*LU) p1.smt2 p2.smt2} 


\newcommand{\class}[1]{\texttt{#1}}


\section{Implementation Details}

In {\skeptik} every logical expression is a simply typed lambda expression, implemented by the abstract class \class{E} with concrete subclasses \class{Var}, \class{App} and \class{Abs} for, respectively, \emph{variables}, \emph{applications} and \emph{abstractions}. Scala's \emph{case classes} are used to make \class{E} behave like an algebraic datatype with (pattern-matchable) constructors \class{Var}, \class{App} and \class{Abs} \`{a} la functional programming.

{\skeptik} is flexible w.r.t. the underlying proof calculus. Every proof node is an instance of the abstract class \class{ProofNode} and must contain a judgment of some subclass \class{J} of \class{Judgment} and a (possibly empty) collection of premises (which are other proof nodes). A proof is a directed acyclic graph of proof nodes; it is implemented as the class \class{Proof}, which provides higher-order methods for traversing proofs. Thanks to Scala's syntax conventions, these methods can be used as an internal domain specific language that integrates harmoniously with the scala language itself.

A proof calculus is a collection of inference rules, implemented as concrete subclasses of \class{ProofNode}. In particular, the main inference rules of the propositional resolution calculus used for representing proofs generated by sat- and smt-solvers are \class{Axiom} and \class{R} (for resolution). They use a \class{Sequent} class (a subclass of \class{Judgment}) to represent clauses.

Classes for expressions and proof nodes are small and correctness conditions are checked during object construction. Once constructed, they cannot be changed, because they are \emph{immutable}. Therefore, incorrect expressions and proofs cannot result from the transformations performed by {\skeptik}. Auxiliary functionality is not implemented in the classes but in their homonymous \emph{companion objects}. Therefore, even though {\skeptik} has more than 21000 lines of code, its most critical core data structures are less than a few hundred lines long.


\section{Supported Proof Formats}
\label{sec:ProofFormats}

Scala's \emph{combinator parsing} library makes it easy to implement parsers for various proof formats. {\skeptik} uses the extension of a file to  determine its proof format. To export proofs in various formats, {\skeptik} provides many exporter classes that extend Java's \class{java.io.Writer} class. Available proof formats are:


\paragraph{TraceCheck Format: }

The TraceCheck \cite{tracecheck} format is one of the three formats accepted at the \emph{Certified Unsat} track of the SAT-Competition and is used by sat-solvers such as \texttt{PicoSAT} \cite{Biere_picosatessentials}. Each line declares a new clause, specifying its name (a fresh positive integer), a space separated list of literals (positive or negative integers, depending on the polarity of the literals), and a list of premises (other clauses, referred to by their names) needed to derive the new clause by regular input resolution. Zero is used as a delimiter. A detailed description can be found at \url{http://fmv.jku.at/tracecheck/}. An example is shown in Figure \ref{TraceCheckProof}.

Other formats accepted at the \emph{Certified Unsat} track are less detailed and hence less convenient to be used by tools that post-process proofs. The omission of premises in the RUP format, for example, throws away information about the DAG structure of the proofs. Nevertheless, there are tools for converting RUP proofs to a resolution format that resembles the trace-check format \cite{van2008verifying}.

%$$
% \mathbf{line} \quad = \quad \mathbf{clause\_name} \ \mathbf{literal}^* \  \mathrm{0} \ \mathbf{premise}^* \  \mathrm{0}
%$$

\begin{figure}[h]
\makebox[\textwidth][c]{
	\qquad\qquad
	\begin{minipage}{.4\textwidth}
		\begin{tikzpicture}
			\rootnode;
			\withchildren{root} {r5}{2}  {r6}{-2};
			\withchildren{r5} {r1}{1,2} {r8}{-1};
			\withchildren{r8} {r2}{-1,2} {r3}{-2};
			\proofnode[above right of = r6]{r4} {1,-2};
			\drawchildren{r6}{r3}{r4};
		\end{tikzpicture}
	\end{minipage}%
	\qquad
	\begin{minipage}{.4\textwidth}
		\begin{tabular}{ccccccc}
			1 & 1  & 2  & 0 & 0 &   & \\
			2 & -1 & 2  & 0 & 0 &   &\\
			3 & -2 & 0  & 0 &   &   &\\
			4 & 1  & -2 & 0 & 0 &   &\\
			5 & 2  & 0  & 1 & 2 & 3 & 0 \\
			6 & -2 & 0  & 3 & 4 & 0 &\\
			7 & 0  & 5 & 6 &   & & \\ 
		\end{tabular}
	\end{minipage}%
	}
	\caption{A proof and its representation in the TraceCheck format}
	\label{TraceCheckProof}
\end{figure}

\paragraph{The SMT-Lib Proof Format: }

{\veriT}

ToDo


\paragraph{{\skeptik}'s Proof Format: }

ToDo

ToDo: with deletion


\section{Proof Compression Algorithms}

Most algorithms for the compression of propositional resolution proofs generated by sat-solvers and smt-solvers described in the literature are available in {\skeptik}. They are shortly described below:

\paragraph{RecyclePivots} (\algo{RP}) \cite{RP08,RP11} compresses a proof by partially regularizing it. A proof is \emph{irregular} \cite{Tseitin} if the resolved literal (pivot) of a resolution proof node is resolved again on the path from this node to the root node. \algo{RP} finds irregular nodes efficiently by traversing the proof from the root to the leaves a single time and memorizing which literals were resolved. When it finds an irregular node, it marks one of its premises for deletion. In a second traversal, from the leaves to the root, irregular nodes are replaced by their non-deleted premises. As full regularization can lead to an exponential blow-up in the proof length \cite{Goerdt}, it is important to regularize carefully and only partially. \algo{RP} achieves this by resetting the set of literals for a node to the empty set when it has more than one child (i.e. when it is the premise of more than one node).


\paragraph{RecyclePivotsWithIntersection} (\algo{RPI}) \cite{LURPI}
differs from \algo{RP} in the treatment of a node with more than one child. Instead of resetting its set of literals to the empty set, the intersection of the sets of literals incoming from its children is computed. In this manner, the exponential blow-up is still avoided, but strictly more irregular nodes are detected and regularized.


\paragraph{LowerUnits} (\algo{LU}) \cite{LURPI} partially eliminates a kind of redundancy that is almost orthogonal to irregularity. When a node $\node$ appears as premise of many resolutions
with the same pivot $p$, it is desirable to resolve $\node$ on $p$ only once instead. This is not always possible, unless $\node$ contains a unit clause (a clause with only the literal $p$).
\algo{LowerUnits} is able to reduce the redundancy by lowering all unit nodes. The nodes are removed from their places and reintroduced in the very bottom of the proof, by a resolving them (at most once) with the root of the fixed proof. 


\paragraph{LowerUnivalents} (\algo{LUniv}) \cite{LUniv} generalizes \algo{LowerUnits}. By
keeping track of nodes that have already been lowered and their pivots, it becomes
possible to lower a non-unit node if it is \emph{univalent}: all its literals but one (its so-called \emph{valent} literal) can be resolved against the valent literals of the already lowered nodes. 


\paragraph{\algo{LUnivRPI}} \cite{LUniv} is a non-sequential combination of \algo{LUniv} after \algo{RPI} and currently provides one of the best trade-offs between compression time and compression ratio. Non-sequential combinations with \algo{LUniv} are easy to implement, because \algo{LUniv} has been implemented as a replacement for the \algo{fixProof} function used by some algorithms to reconstruct a proof after deletions. 
%

\paragraph{\algo{RPI[3]LU} and \algo{RPI[3]LUniv}} are non-sequential combinations of \algo{RPI} after \algo{LU}
and \algo{LUniv}, respectively. They consist of three traversals. The first traversal collects subproofs to be
lowered. The second traversal computes the sets of safe literals for each node,
taking into account the subproofs marked for being lowered. The last traversal actually compresses the proof by removing redundant branches and lowering subproofs. These algorithm are optimizations of the corresponding sequential compositions, achieving the
same compression ratio in less time. 


\paragraph{Reduce\&Reconstruct} (\algo{RR}) \cite{RedRec} applies local transformation
rules that either eliminate local redundancies or shuffle the order of resolution steps (similarly to what Gentzen's rank reduction rules for cut-elimination do) in order to gradually transform non-local redundancies into local ones. Although the set of local rules used are sufficient to emulate any
other compression algorithm, the algorithm may need many traversals to shuffle the order of resolutions steps sufficiently well to eliminate non-local redundancies. This algorithm can achieve very good
compression ratio, if run for a long enough time.
%
The implementation in \skeptik is very modular, allowing convenient experimentation with various alternative local transformation rules, heuristics to apply
them and termination conditions. There still seems to be plenty of space to improve heuristics and termination conditions for this algorithm.


\paragraph{Split} \cite{CottonSplit} lowers pivot variables in a proof. 
From a proof with conclusion $C$, two proofs with conclusions $v \vee C$ and $\neg{v} \vee C$ are constructed,  where the variable $v$ is chosen heuristically.
In a first step the positive/negative premises of resolvents with pivot $v$ are removed from the proof.
Afterwards the proof is fixed, by traversing it top-down and fixing each proof node.
A proof node is fixed by either replacing it by one of its fixed premises or resolving them.
The roots of the resulting proofs are resolved, using $v$ as pivot, to obtain a new proof of $C$.
The time-complexity of this algorithm is linear in the proof length, but it has to be repeated many times to obtain significant compression. This can be done iteratively or recursively. Also multiple variables can be chosen in advance. 
All these variants are implemented in {\skeptik}.

\paragraph{Tautology Elimination} (\algo{ET}) eliminates proof nodes containing tautological clauses. Although tautological clauses normally do not occur in proofs generated by sat- and smt-solvers, they may occur in proofs compressed by some of the algorithms described here.


\paragraph{DAGification} (\algo{D}) finds proof nodes having equal clauses and replaces each of them by only one of them. 


\paragraph{Subsumption} algorithms generalize DAGification by using the subsumption relation on clauses instead of the equality relation. A clause $C_1$ subsumes a clause $C_2$ iff every literal occurring in $C_1$ also occurs in $C_2$. The goal is to replace a node containing a clause $C_2$ by another node containing a clause $C_1$ if $C_1$ subsumes $C_2$.
There are three subsumption-based proof compression algorithms implemented in Skeptik. 
%
\algo{TopDownSubsumption} (\algo{TDS}) searches for subsumed clauses among all clauses visited earlier in a top-down traversal. The time-complexity of this algorithm is worst case quadratic in the number of proof nodes.
%
\algo{BottomUpSubsumption} (\algo{BUS}) searches for subsumed clauses among all clauses visited earlier in a bottom-up traversal. A subsumed clause $D$ can only replace a clause $C$, if $D$ is not an ancestor of $C$ in the graph representing the proof. \algo{BUS} has the same time-complexity as \algo{TDS}, but the additional ancestor-check makes it much slower in practice.
%
\algo{RecycleUnits} (\algo{RU}) \cite{RP11} is a special case of \algo{BUS} that only searches for subsuming clauses that are unit (i.e. contain only one literal). Its worst case time-complexity is quadratic in the number of unit clauses.

\paragraph{Pebbling} algorithms \cite{ToDo: cite submitted paper} compress proofs w.r.t. their \emph{space}, not their length. The space of a proof is the maximum number of proof nodes that have to be kept in memory simultaneously, while reading and checking the proof.\\
This measure is closely related to the \textbf{Black Pebbling Game} \cite{gilbert1980pebbling}, which lends its name to the algorithms described here. A strategy for this game is a topological node ordering (i.e. an ordering of nodes such that every premise of each node is lower than the node itself) enriched with deletion information (i.e. extra lines in the proof output indicating that a node can be deleted from memory). An optimal strategy for the Black Pebbling Game would therefore result in optimal space compression of the proof. 
However, as shown in \cite{gilbert1980pebbling}, deciding whether a proof can be pebbled using at most $n$ pebbles is PSPACE-complete, so constructing the optimal solution is not a feasable approach for \skeptik. Greedy heuristics are used for finding a good, though not optimal, node ordering.
\algo{TopDownPebbler} (\algo{TDP}) plays the game in a top-down manner. At every point there are nodes which can be pebbled. Initially only the axioms can be pebbled. Using information from the pebblable nodes and their children, heuristics decide which node to pebble next. If all premises of a node have been pebbled, it becomes pebblable. This is repeated until the root node is pebbled. Unfortunately, \algo{TDP}'s heuristics have very limited knowledge about the local structure of the proof only and is often not able to achieve good compression.
\algo{BottomUpPebbler} (\algo{BUP}) constructs a node ordering by visiting proof nodes and their premises recursively, starting from the root node. At every node, \algo{BUP} heuristically chooses which subproof rooted in the premises of the node should be pebbled first. After all premises are pebbled, the node itself is pebbled. \algo{BUP} is thus more aware of the global structure of the proof, and achieves much better space compression than \algo{TDP}.



\section{Conclusions and Future Work}

ToDo: limitation: memory consumption in Scala
underlying symbols are strings

New proof formats on demand

extension to first-order

contextual natural deduction

cut-introduction

Vienna Scientific Cluster

\bibliographystyle{splncs}
\bibliography{biblio}


\end{document}

% vim: tw=100
