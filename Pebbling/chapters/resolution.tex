\section{Propositional Resolution Proofs}
\label{sec:resolution}

Resolution is among the most prominent formal calculi for automated deduction and goes back to Robinson \cite{Robinson1965}.
Propositional resolution can be seen as a simplification of first-order logic resolution to propositional logic.
For basics about propositional logic and its prominent decision problem SAT, we refer the reader to \cite{Biere2009}.
For an extensive discussion of propositional and first-order logic resolution, we refer the reader to \cite{Leitsch1997}.

\begin{definition}[Literal and Clause]

A \emph{literal} is a propositional variable or the negation of a propositional variable. 
The \emph{complement} of a literal $\ell$ is denoted $\dual{\ell}$ (i.e. for any propositional variable $p$,
$\dual{p} = \neg p$ and $\dual{\neg p} = p$). 
A \emph{clause} is a set of literals. 
$\bot$ denotes the \emph{empty clause}.

\end{definition}

A clause represents the propositional logic formula that is the disjunction of its literals.
A set of clauses represents the formula that is the conjunction of its clauses.
The propositional resolution calculus derives new clauses using the propositional resolution rule with the aim of deriving the empty clause.

\begin{definition}[Resolvent]
\label{def:resolvent}
Let $C_1$ and $C_2$ be two different clauses and $\ell$ be a literal, such that $\ell \in C_1$ and $\dual{\ell} \in C_2$.
The clause $C_1 \setminus \{\ell\} \cup C_2 \setminus \{\dual{\ell}\}$ is called the \emph{resolvent} of $C_1$ and $C_2$ with \emph{pivot} $\ell$.

\end{definition}

The condition of $C_1$ and $C_2$ being different technically is not necessary.
However if it is possible to resolve a clause with itself, then the clause contains both the positive and negative version of a variable and is therefore tautological (i.e. trivially satisfiable).
Since the resolution calculus is refutational, i.e. it seeks to show unsatisfiability, such clauses are of no use and therefore we ignore them.
In case it is possible to produce a resolvent of two clauses w.r.t. two different literals, no matter which literal is chosen, the resulting resolvent will be tautological.
Therefore we will drop the reference to the literal when speaking about resolvents.

It is usual to present proofs in this calculus as syntactic derivations and refutations, that are sequences of clauses.
However, in this work, we investigate the graph structure of proofs and therefore present proofs as graphs in the following definition.

\begin{definition}[Proof] 
\label{def:proof}
A \emph{proof} $\varphi$ is a labeled directed acyclic graph $\langle V,E,\n,\mathcal{L} \rangle$, such that
$\n$ is a unique root of the graph, that is $\n$ has no incoming edges and every node is reachable from $\n$, $\mathcal{L}$ maps nodes to clauses and one of the following properties is fulfilled:

\begin{enumerate}
	\item $V = \{\n\}, E = \emptyset$
	\item \label{enum:resCase} 
		There are proofs $\varphi_L = \langle V_L, E_L, \n_L,\mathcal{L}_1 \rangle$ and $\varphi_R = \langle V_R, E_R, \n_R, \mathcal{L}_2 \rangle$ such that 
		$\n \notin (V_L \cup V_R)$, $\mathcal{L}_1(x) = \mathcal{L}_2(x)$ for every $x \in (V_L \cap V_R)$,
		$\mathcal{L}(\n)$ is the resolvent of $\mathcal{L}(\n_L)$ and $\mathcal{L}(\n_R)$ w.r.t. some literal $\ell$,
		for $x \in V_L: \mathcal{L}(x) = \mathcal{L}_1(x)$ and for $x \in V_R: \mathcal{L}(x) = \mathcal{L}_2(x)$,
		$V = (V_L \cup V_R) \cup \{\n \}$, $E = E_L \cup E_R \cup \{(\n_L,\n) , (\n_R,\n) \}$.
\end{enumerate}

For a node $v \in V$, $\mathcal{L}(v)$ is the \emph{conclusion} of $v$.
In case \ref{enum:resCase} $\n_L$ and $\n_R$ are \emph{premises} of $\n$ and $\n$ is a \emph{child} of $\n_L$ and $\n_R$.
A proof $\psi$ is a \emph{subproof} of a proof $\varphi$, if the respective roots are related in the transitive closure of the premise relation.
The root of a subproof $\psi$ of $\varphi$ which has no premises is an \emph{axiom} of $\varphi$.
$\Axioms{\varphi}$ denotes the set of axioms of $\varphi$. 
$\Premises{v}{\varphi}$ denotes the premises and $\Children{v}{\varphi}$ the children of a node $v$ in a proof $\varphi$.

\end{definition}

Note that since the labeling of premises must agree on common nodes and edges, the definition of the labeling $\mathcal{L}$ is unambiguous.
Also note that in case \ref{enum:resCase} of Definition \ref{def:proof} $V_L$ and $V_R$ are not required to be disjoint. 
Therefore the underlying structure of a proof is really a directed acyclic graph and not simply a tree. 
Modern SAT- and SMT-solvers, using techniques of conflict driven clause learning, produce proofs with a  DAG structure \cite{Bouton2009,Biere2009}.
The reuse of proof nodes plays a central role in proof compression \cite{Fontaine2011}.

Several measures can be defined on proofs.
The relevant measure for this work is space, which is defined in Section \ref{sec:pebbling-game}.
Other common measures of proofs that are not discussed in this work are for example length, height, width and size of the unsat core.


\begin{example}

Consider the propositional logic formula $\Phi$, displayed in clause notation.
$$\Phi := {\langle \{x_1, x_2, \neg x_3\}, \{x_1, \neg x_2\}, \{x_1, x_3\}, \{\neg x_1\} \rangle}$$
%$$\Phi := (x_1 \vee x_2 \vee \neg x_3) \wedge (x_1 \vee \neg x_2) \wedge (x_1 \vee x_3) \wedge (\neg x_1)$$
%In clause notation of this ${\langle \{x_1, x_2, \neg x_3\}, \{x_1, \neg x_2\}, \{x_1, x_3\}, \{\neg x_1\} \rangle}$.
By resolving the clauses $\{x_1, x_2, \neg x_3\}$ and $\{x_1, \neg x_2\}$, we obtain the clause $\{x_1,\neg x_3\}$, which we can resolve with $\{x_1, x_3\}$ to obtain $\{x_1\}$.
Finally, we obtain the empty clause $\bot$ by resolving $\{x_1\}$ with $\{\neg x_1\}$,
which proofs that $\Phi$ is unsatisfiable.
The resulting proof is displayed in Figure \ref{fig:resolutionexample}.

\begin{figure}[!h]

\input{figures/resolutionexample}
\caption{Proof of $\Phi$'s unsatisfiability}
\label{fig:resolutionexample}
\end{figure}

%\begin{figure}[!h]

%\input{chapters/proofcompression/figures/resolutionexample2}
%\caption{Another Proof of $\Phi$'s unsatisfiability}
%\label{fig:resolutionexample2}
%\end{figure}

\end{example}

The aim of this work is to make proof processing easier by minimizing space requirements of proofs.
Proof processing could be checking its correctness, manipulating it, as we do in this work extensively, or extracting information, for example interpolants and unsat cores, from it.
The following definition makes the notion of proof processing formal.

\begin{definition}[Proof Processing]
\label{def:proof-processing}

Let $\varphi = \langle V,E,\n,\mathcal{L} \rangle$ be a proof and $T$ be an arbitrary set.
A function $f: V \times T \times T \rightarrow T$ is a \emph{processing function} if there is a function $g_f: V \rightarrow T$ such that for every $v \in \Axioms{\varphi}: g_f(v) = f(v,t_1,t_2)$ for all $\{t_1,t_2\} \subseteq T$.
Let $\mathcal{F}$ be the set of processing functions.
The \emph{apply function} $\ap: V \times \mathcal{F} \rightarrow T$ is defined recursively as follows.
$$
\ap(v,f) = 
\begin{cases}
	f(v,\ap(p_1,f),\ap(p_2,f)) &\text{if } \Premises{v}{\varphi} = \{p_1,p_2\} \\
	g_f(v) &\text{otherwise}\\
\end{cases}
$$

\emph{Processing a node} $v$ with some processing function $f$ means computing the value $\ap(v,f)$.
\emph{Processing a proof} means to process its root node.

\end{definition}

\begin{example}

Checking the correctness of some proof (i.e. checking for the absence of faulty resolution steps) can be done in terms of the following processing function with $T = \{\top,\bot\}$ and $\wedge$ being the usual boolean and-operation.
$$
f(v,w_1,w_2) = 
\begin{cases}
	\top & \text{if } v \text{ has no premises} \\
	%w_1 \wedge w_2 &\text{if } v \text{ has premises }\\
								 %&\quad pr_1, pr_2 \text{ and}\\
								 %&\quad \mathcal{L}(v) \text{ is the resolvent of}\\
								 %&\quad \mathcal{L}(pr_1) \text{ and } \mathcal{L}(pr_2)\\
	w_1 \wedge w_2 &\text{if } \Premises{v}{\varphi} = \{p_1,p_2\} \text{ and}\\
								 &\quad \mathcal{L}(v) \text{ is a resolvent of} \\
								 &\quad \mathcal{L}(p_1) \text{ and } \mathcal{L}(p_2)\\
	\bot & \text{ otherwise}
\end{cases}
$$

Processing a proof with processing function $f$ yields $\top$ if and only if the proof is a correct resolution proof.
\end{example}

Another example of proof processing is any proof compression algorithm, that might alter the structure of a proof.
In that case, the set $T$ contains (potentially new) proof nodes.