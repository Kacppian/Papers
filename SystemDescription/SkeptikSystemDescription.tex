\documentclass{llncs}

\usepackage{xcolor}
\usepackage{enumitem,amsmath,amssymb}
\usepackage{breakurl}    % used for \url and \burl
\usepackage[linesnumbered,boxed,noline,noend]{algorithm2e}
\def\defaultHypSeparation{\hskip.1in}

\usepackage{tikz}
\usepackage{subfig}
\usepackage{array,booktabs,multirow}
\usepackage{placeins}

\usepackage{logictools}
\usepackage{prooftheory}
\usepackage{comment}
\usepackage{mathenvironments}
\usepackage{drawproof}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}

\title{The \skeptik Proof Compression System}

\author{
  Joseph Boudou\inst{1}
  \thanks{Supported by the Google Summer of Code 2012 program.}
  \and 
  Andreas Fellner\inst{2,3}
  \thanks{Supported by the Google Summer of Code 2013 program.}
  \and 
  Bruno Woltzenlogel Paleo\inst{3}
  \thanks{Supported by the Austrian Science Fund, project P24300.}
}

\authorrunning{J.\~Boudou \and A. Fellner \and B.\~Woltzenlogel Paleo}

\institute{
  IRIT, Universit\'e de Toulouse, France \\
  \email{joseph.boudou@irit.fr}
  \and 
  Free University of Bolzano, Italy \\
  \email{fellner.a@gmail.com}
  \and 
  Vienna University of Technology, Austria \\
  \email{bruno@logic.at}
}




\begin{document}

\maketitle


\begin{abstract}
This paper describes the current state of \skeptik: a system for checking, compressing and improving proofs obtained by automated reasoning tools.
\end{abstract}

\setcounter{footnote}{0}


\section{Introduction}

There are various reasons why it is highly desirable 
that automated reasoning
tools output not only a \emph{yes} or \emph{no} answer to a problem 
but also \emph{proofs/refutation} or \emph{(counter)models}. 

Firstly, state-of-the-art tools are complex and often heavily optimized. 
Their code is hard to understand and difficult to automatically verify.
Therefore, it is reasonable to distrust their answers. 
But if proofs or (counter)models are produced, 
they can serve as independently checkable certificates of 
correctness for the provided answers. 

Secondly, for most applications, a \emph{yes/no} is inherently insufficient. We often already known in advance whether a problem is expected to be satisfiable or unsatisfiable, and we want more than just a confirmation of this expectation. For satisfiable formulas, it is the model that encodes the desired information; for valid formulas, it is the proof that contains the desired information. In case the expectation was wrong, refutations and countermodels can be very helpful to explain issues in the encoding of the problem, in order to correct and refine it.

Proofs and refutations have been used, for example, to extract unsatisfiable cores \cite{ToDo}, interpolants \cite{}, Herbrand disjunctions \cite{}, implicatures \cite{} \ldots

Although current automated deduction tools are very efficient at finding proofs, they do not necessarily find the best proofs. {\skeptik}\footnote{\url{http://github.com/Paradoxika/Skeptik/}} aims precisely at efficiently analyzing proofs, in order to eliminate their redundancies. The proofs thus compressed and improved by Skeptik can better serve the needs of applications using proofs.


\paragraph{Related Work} ToDo: mention CERes and GAPT


\section{Usage}



\section{Supported Proof Formats}
\label{sec:ProofFormats}

ToDo: RUP

\subsection{TraceCheck Format}

ToDo (Andreas)

\subsection{The SMT-Lib Proof Format}

{\veriT}

ToDo (Bruno)

\subsection{{\skeptik}'s own Proof Format}

ToDo (Joseph)


\section{Available Proof Compression Algorithms}

\subsection{RecyclePivots and RecyclePivotsWithIntersection}

In \cite{Tseitin}, Tseitin outlines a first kind of proof redundancy with the concept of
\emph{irregular proof}. A proof is irregular if some pivot is resolved more than once on some path
from the root to a leaf. But Goerdt \cite{Goerdt} proved that the corresponding regular proof of an
irregular proof may be exponentialy bigger than the original. Therefore, only \emph{partial}
regularisation algorithm can compress a proof.

\algo{RecyclePivots} (\algo{RP}) \cite{RP08,RP11} and \algo{RecyclePivotsWithIntersection}
(\algo{RPI}) \cite{LURPI} are such partial regularisation algorithms. They both compute for each
node a set of \emph{safe literals} and then delete a premise iff its auxiliary literal is safe. The
set of safe literals is constructed recursively from the root of the proof by adding the dual of the
auxiliary literal. The two algorithms only differs for nodes which are premises of more than one
resolution. In that case, \algo{RP} resets the safe literals to the empty set whereas \algo{RPI}
computes the intersection of the safe literals corresponding to each incoming edge.

\subsection{LowerUnits}

Another kind of proof redundancy happens when a node $\node$ appears as premise of many resolutions
on the same pivot $p$. In that case, it would be better to resolve $\node$ on $p$ only once. The
\algo{LowerUnits} (\algo{LU}) algorithm \cite{LURPI} reduces this kind of redundancy by lowering
\emph{units} down to the root of the proof. Units are subproofs whose conclusion consists in a
single literal. Such a subproof can always be lowered down the proof.

\algo{LowerUnits} is of linear time complexity in the length of the proof. It is a very fast
algorithm which perform good compression ratio.

\subsection{LowerUnivalents}

The \algo{LowerUnivalents} algorithm \cite{LUniv} (\algo{LUniv}) generalizes \algo{LowerUnits} by
exploiting the information of the already lowered subproofs and their pivots. Then, it becomes
possible to lower a non-unit node if all its conclusion's literals but one can be deleted by the
already lowered subproofs. Moreover, the lowered pivots are safe literals and thus some partial
regularization can be achieved simultaneously.

In \skeptik, \algo{LUniv} has been implemented as a replacement for the \algo{fixProof} function
used by some algorithms to reconstruct a proof after deletions. Therefore, non-sequential
combinations of those algorithms with \algo{LUniv} are easy to implement. For instance,
\algo{LUnivRPI} is a non-sequential combination of \algo{LUniv} after \algo{RPI}. This latter
algorithm is currently one the best trade-off between compression time and compression ratio
\cite{LUniv}.

\subsection{RPI[3]LU and RPI[3]LUniv}

\algo{RPI[3]LU} and \algo{RPI[3]LUniv} are non-sequential combinations of \algo{RPI} after \algo{LU}
and \algo{LUniv}. They consist in three traversals. The first traversal collects subproofs to be
lowered down the proof. The second traversal computes the sets of safe literals for each node,
taking into account the subproofs marked for being lowered. The last traversal actualy compress the
proof by removing redundant branches and lowering subproofs.

These algorithm are optimizations of the corresponding sequential compositions, achieving the
same compression ratio in less time. 

\subsection{ReduceAndReconstruct}

The \algo{ReduceAndReconstruct} (\algo{RedRec}) approach consists in applying local transformation
rules to each node. The set of local rules presented in \cite{RedRec} are sufficient to emulate any
other compression algorithm. Unfortunately, the proposed heuristic only consists in trying to apply
each rule (with given priorities) to each node in a top-down traversal. For this heuristic to be
efficient, the process has to be repeated many times. The resulting algorithm can achieve very good
compression ratio if run long enough.

As this algorithm allows experimentations in the local transformation rules, the heuristic to apply
them and the termination conditions, its implementation in \skeptik is very modular. Each component
is defined independently and a convenient framework allows to combine them as desired. A handful of
alternative local transformation rules and termination conditions have been implemented too.

\subsection{Split}

The Split \cite{CottonSplit} algorithm is a technique to lower pivot variables in a proof.\\
From a proof with conclusion $C$, two proofs with conclusion $v \vee C$ and $\neg{v} \vee C$ are constructed,  where the variable $v$ is chosen heuristically.\\
In a first step the positive/negative premises of resolvents with pivot $v$ are removed from the proof.
Afterwads the proof is fixed, by traversing it top-down and fixing each proof node.
A proof node is fixed by either replacing it by one of or resolving the fixed premises.\\ %I'm not sure if the end of this sentence is elegant or unreadable
The roots of the resulting proofs are resolved, using $v$ as pivot, to obtain a new proof of $C$.\\
The time-complexity of this algorithm is linear in the proof length %this should be true, right? but the original paper doesn't state this
and it suits very well for repeated application. This can be done iteratively or recursively. Also multiple variables can be chosen in advance. 
All these variants are implemented into Skeptik.

\subsection{TautologyElimination}

\subsection{StructuralHashing}

\subsection{DAGification}

\subsection{Subsumption}

Subsumption based algorithms use, as the name implies, the subsumption relation on clauses for compressing a proof. A clause $C_1$ subsumes a clause $C_2$ \textit{iff} every literal occuring in $C_1$ also occurs in $C_2$.
The general goal is to replace a clause $C$ by another clause $D$, used elsewhere in the proof, that are such that $C$ subsumes $D$.
There are three subsumption based compression algorithms implemented into Skeptik.\\
\textbf{Top-down Subsumption} searches for subsumed clauses among all clauses visited earlier in a top-down traversal. The time-complexity of this algorithm is worst case quadratic in the number of proof nodes.\\
\textbf{Bottom-up Subsumption} searches for subsumed clauses among all clauses visited earlier in a bottom-up traversal. A subsumed clause $D$ can only replace a clause $C$, if $D$ is not an ancestor of $C$ in the graph representing the proof. Bottom-up Subsumption has the same time-complexity as Top-down Subsumption, but the additional ancestor-check makes it slower in practise.\\
\textbf{RecycleUnits} \cite{RP11} is a special case of Bottom-up Subsumption that only searches for subsumed clauses that are unit (i.e. contain only one literal). This algorithm has a worst case time-complexity that is quadratic in the number of unit clauses.

\subsection{Pebbling}

Pebbling algorithms compress proofs in the \textbf{space measure}, as opposed to the usual length compression. The space measure of a proof indicates how many proof nodes maximally have to be kept in memory, while reading the proof, simultaneously.\\
This measure is closely related to the \textbf{Black Pebbling Game} \cite{gilbert1980pebbling}, which lends its name to the algorithms described here. A strategy for this game directly corresponds to a topological node ordering, i.e. an ordering of nodes such that every premise of each node is lower than the node itself, combined with deletion information, i.e. extra lines in the proof output indicating that a node can be deleted from memory. An optimal strategy for the Black Pebbling Game would therefore result in optimal space compression of the proof. However, as shown in \cite{gilbert1980pebbling}, finding the optimal solution is PSPACE-complete and therefore not a feasable approach for this program.\\
To obtain algorithms that have an acceptable runtime greedy heuristics for finding a good node ordering are used.\\
\textbf{Top-down pebbling} directly corresponds to playing the game with limited information on how the proof looks. At every point there are nodes which can be pebbled (initially these are only the axioms). Using information from these nodes and their children nodes, it is decided which node to pebble next, which can make other nodes pebblable. This is done until the root node is pebbled. Unfortunately the lack of knowledge about the structure of the proof often results in bad space compression.\\
\textbf{Bottom-up pebbling} constructs a node ordering by visiting proof nodes and their premises recursively, starting from the root node. At every node it is chosen heuristically in what order its premises are visited. After all premises are visited, the node is added to the oder.

\section{Implementation Details}

ToDo: statistics about the code

\subsection{Organization of the Code}

ToDo: brief description of the package structure


\subsection{Data Structures for Formulas}


\subsection{Data Structures for Proofs}

immutable


\section{Conclusions and Future Work}

ToDo: limitation: memory consumption in Scala
underlying symbols are strings

New proof formats on demand

extension to first-order


\bibliographystyle{splncs}
\bibliography{biblio}


\end{document}

% vim: tw=100
