\section{Introduction}

Proofs have the potential to play an important role in ensuring trust
between independent software tools that transfer knowledge, solutions
and computation results among each other, because proofs can serve as
certificates of correctness of what is communicated. This potential is
already being realized in the field of automated reasoning, where
proof exchange for theorem proving
\cite{PxTP2013,PxTP2015} is routinely used to
enable verified cooperation among a vast variety of proof-producing
automated deduction tools \cite{APPA}, as exemplified in systems such
as Sledgehammer \cite{paulson2010three} and
HOLyHammer\cite{zbMATH06419295}. 

SAT- and SMT- Solvers are widely used as subroutines to solve problems in various domains.
Clearly solvers are not immune to bugs \cite{brummayer2009fuzzing,brummayer2010automated} that may lead to wrong results.
Fortunately, there is a simple way to certify the output of such solvers and 
prevent the potentially dangerous propagation of an error in them.

In case of the positive result ``satisfiable formula'', the certificate is a model of the formula.
Checking a model for spuriousness can be done by substituting the variables of the formula with the respective values in the model and 
calculating the truth value of the resulting variable free formula.

In case of the negative result ``unsatisfiable formula'', the certificate is a proof of unsatisfiability.
Checking a proof for spuriousness, can be done by replaying the proof and validating the correct application of every proof rule.
Typically this is a much more expensive task than checking whether a model is spurious.
%Typically, a proof is much larger than a model and thereby checking the proof is a much more expensive task than checking the correctness of a model.
Nevertheless, to encourage certification of unsatisfiability, the yearly SAT competition\footnote{\url{http://baldur.iti.kit.edu/sat-competition-2016/}} has introduced a Certified UNSAT track in 2013.

%\marginpar{ToDo: throughout the paper, the QED (box) symbol is often appearing after a line break, in the left side. It should appear in the right side without a line break.}
An example of application where certified correctness is crucial is the verification and synthesis of software \cite{Biere2009,Hofferek2013}. 
%SAT- and SMT-solvers are among the most efficient tools for this task, but the proofs they generate can be huge. 
%Checking their correctness or extracting information (e.g. unsatisfiable cores, interpolants) from them can not only take a long time but also consume a lot of memory. 
In an ongoing project for interpolant-based controller synthesis \cite{Hofferek2013}, 
for example, extracting an interpolant from an SMT-proof took hours and reached the limit of memory (256GB) available in a single node of the computer cluster used in the project. 
The example shows one serious issue with proofs: they tend to be huge, easily filling up all available memory, and therefore hard to process independently.
%This issue is relevant in any application scenario in which proof consumers, who are interested in independently processing proofs, might have less available memory than the proof producer.

Typically, proof formats do not allow proof producers to inform the proof consumer when proof nodes (containing clauses) could be released from memory. Consequently, every proof node loaded into memory has to be kept there until the whole proof is completely processed, because the proof consumer does not know whether the proof node will still be needed. To address this issue, recently proposed formats for SAT-proofs such as DRAT and BDRUP enrich the older RUP format with node deletion instructions \cite{raey}. Other proof formats, such as the TraceCheck format \cite{BiereHeuleAPPA} or formats for SMT-proofs \cite{BarrettFontaineMouraAPPA}, could also be enriched analogously. When generating a proof file eagerly (i.e. writing learned clauses immediately to the proof file) a SAT- or SMT-solver can add a deletion instruction for every clause that is deleted by the periodic clean-up of its database of derived learned clauses.

This paper explores the possibility of post processing a proof in order to increase the amount of deletion instructions in the proof file. 
The more deletion instructions, the less memory the proof consumer will need.

%Therefore, this \emph{deletion-during-proof-postprocessing} approach ought to be seen not as a replacement but rather as an independent complement to the \emph{deletion-during-proof-search} already performed by state-of-the-art proof-generating SAT-solvers.

The new methods proposed here exploit an analogy between proof checking and playing \emph{Pebbling Games} \cite{Kasai1979,Gilbert1980}.
In Section \ref{sec:resolution} we define propositional resolution proofs and make the notion of processing a proof formal.
The particular version of pebbling game relevant for proof processing is defined precisely in Section \ref{sec:pebbling-game}, where we also explain the analogy to proof processing in detail and define the space measure of proofs. The proposed pebbling algorithms are greedy (Section \ref{sec:algorithms}) and based on heuristics (Section \ref{sec:heuristics}). As discussed in Sections \ref{sec:pebbling-game} and \ref{sec:pebblingSAT}, approaches based on exhaustive enumeration or on encoding as a SAT problem would not fare well in practice.

The proof space compression algorithms described here are not restricted to proofs generated by SAT- and SMT-solvers. They are general DAG pebbling algorithms, that could be applied to proofs represented in any calculus where proofs are directed acyclic graphs (including the special case of tree-like proofs) \cite{APPA}. It is nevertheless in SAT and SMT that proofs tend to be largest and in most need of space compression. The underlying propositional resolution calculus satisfies the DAG requirement. The experiments (Section \ref{sec:experiments}) evaluate the proposed algorithms on thousands of SAT- and SMT-proofs.

\subsection{Related Work}

To the best of our knowledge, the methods presented here are the first to compress the \emph{space} of proofs. Nevertheless, there have been many works aimed at compressing the \emph{length} or the \emph{size} of proofs in various proof calculi. 

In the case of sequent calculus proofs, Herbrand sequent extraction \cite{B16,B12,B51} was one of the earliest proposed techniques. 
It consists of obtaining a propositionally valid sequent from a first-order proof with the same propositional structure of the proof's end-sequent but with all needed instantiations for the quantified variables. 
This technique was later generalized to higher-order logic \cite{DBLP:journals/corr/LibalRR14} using expansion trees \cite{DBLP:books/daglib/0036008}, which is a more suitable data structure than sequents for higher-order logic. A more ambitious approach to compressing sequent calculus proofs is the introduction of cuts. As every cut inference involves the derivation of a lemma (in its left premise) and its use (in the right premise), introducing cuts requires solving the difficult task of synthesizing lemmas. The first method \cite{BrunoLPAR} to address this problem introduced atomic cuts by using the resolution calculus, which is based on atomic cuts. A few years later, another method \cite{Hetzl}, based on discovering a grammar that could generate the Herbrand sequent of the proof to be compressed and then constructing a proof with cuts based on that grammar, was also proposed and implemented in \texttt{GAPT} \cite{GAPT}. 

More recently, for natural deduction proofs, which are common among proof assistants, the only known technique involves reproving the theorem in \emph{Contextual Natural Deduction} \cite{DBLP:conf/lfcs/Paleo13,DBLP:conf/ershov/Paleo15}, which allows at least quadratic asymptotic best-case compression. However, this technique is still limited to minimal logic only and need to be extended to more complex logics in order to be practically useful.

It is in the case of propositional resolution, which enjoys high popularity among SAT-solvers and SMT-solvers, that proof compression algorithms have been most studied and applied. 
The first two algorithms for compressing propositional resolution proof length were \emph{RecycleUnits} and \emph{RecyclePivots} \cite{Bar-Ilan2008}, with the latter consisting of making the proof more \emph{regular} by deleting parents of irregular nodes when possible. 
This algorithm was later improved with the invention of \emph{RecyclePivotsWithIntersection} \cite{Fontaine2011}, which discovered and used a more general way of detecting deletable irregularities. Combinations of this improved algorithm with a novel technique called \emph{LowerUnits} \cite{Fontaine2011} were also described and evaluated there. 
\emph{LowerUnits} was then also improved further by postponing resolutions not only with unit clauses, but also with non-unit clauses satisfying \emph{univalence} conditions. 
The resulting algorithm was called \emph{LowerUnivalents} \cite{DBLP:conf/tableaux/BoudouP13}. 
All these algorithms traverse the proof only a constant number of times. 
However, there are also algorithms that do not have this property. 
The most well-known are \emph{Split} \cite{cotton2010two}, which splits the refutation into a proof $p$ and a proof $\neg p$ for an heuristically chosen atom $p$ and then recombines the two proofs by resolving $p$ and $\neg p$ into a hopefully smaller new refutation, and \emph{ReduceAndReconstruct} \cite{DBLP:conf/hvc/RolliniBS10}, which looks for local patterns of redundancy and locally rewrites them, occasionally also shuffling the order of proof nodes to expose new local patterns. 
Both algorithms can be iterated an unbounded number of times, and the amount of compression they achieve typically depends on how many iterations are executed.

\emph{LowerUnits} has been generalized from propositional- to first-order resolution \cite{DBLP:conf/cade/GorznyP15}, which can be considered a popular foundation for first-order superposition-based automated theorem provers. 
The algorithms \emph{Split} and \emph{RecyclePivotsWithIntersection} are currently being generalized to first-order logic as well.

All natural deduction, propositional and first-order resolution proof compression algorithms mentioned have been implemented by various people in \texttt{Skeptik} \cite{Boudou2014}, whereas the sequent calculus algorithms have been implemented either in the \texttt{CERes} system \cite{DBLP:conf/cade/DunchevLLWP10} for cut- elimination by resolution, or in its successor \texttt{GAPT} \cite{GAPT} (the General Architecture for Proof Theory).

Although there has never been a proof compression algorithm targeting \emph{space}, instead of \emph{length} or \emph{size}, theoretical studies about trade-offs between these and other measures have been investigated in the related field of propositional proof complexity (e.g. \cite{Ben-Sasson2002}). The theoretical works in that neighboring field inspired us to develop constructive methods to obtain a proof with low space, as presented here. One important subtlety is that here we consider traversal orders of a fixed proof instead of constructing a completely new proof. The motivation for this restriction is that proof compression should ultimately deliver proofs with low size and low space at the same time. We are not interested in reducing space further at the expense of increasing size or length.

Another inspiration for our work were new proof formats such as DRUP \cite{DRUP} and DRAT \cite{raey}, which allow the proof producer to explicitly highlight at which point proof nodes can be deleted from memory because they will not be used anymore. We aim to construct proof traversal orders that have these deletion points as beneficial as possible for peek memory consumption.
