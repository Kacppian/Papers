\documentclass{llncs}

\usepackage{xcolor}
\usepackage{enumitem,amsmath,amssymb}
\usepackage{breakurl}    % used for \url and \burl
\usepackage[linesnumbered,boxed,noline,noend]{algorithm2e}
\def\defaultHypSeparation{\hskip.1in}

\usepackage{tikz}
\usepackage{subfig}
\usepackage{array,booktabs,multirow}
\usepackage{placeins}

\usepackage{graphicx}
\usepackage{wrapfig}

\usepackage{logictools}
\usepackage{prooftheory}
\usepackage{comment}
\usepackage{mathenvironments}
\usepackage{drawproof}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}

\title{ 
Skeptik [System Description]
}

\author{
  Joseph Boudou\inst{1}
  \thanks{Supported by the Google Summer of Code 2012 program.}
  \and 
  Andreas Fellner\inst{2,3}
  \thanks{Supported by the Google Summer of Code 2013 program.}
  \and 
  Bruno Woltzenlogel Paleo\inst{3}
  \thanks{Supported by the Austrian Science Fund, project P24300.}
}

\authorrunning{J.\~Boudou \and A. Fellner \and B.\~Woltzenlogel Paleo}

\institute{
  IRIT, Universit\'e de Toulouse, France \\
  \email{joseph.boudou@irit.fr}
  \and 
  Free University of Bolzano, Italy \\
  \email{fellner.a@gmail.com}
  \and 
  Vienna University of Technology, Austria \\
  \email{bruno@logic.at}
}




\begin{document}

\maketitle


\begin{abstract}
This paper introduces \skeptik: a system for checking, compressing and improving proofs obtained by automated reasoning tools.
\end{abstract}

\setcounter{footnote}{0}


\section{Introduction}

There are various reasons why it is desirable 
that automated reasoning
tools output not only a \emph{yes} or \emph{no} answer to a problem 
but also \emph{proofs/refutations} or \emph{(counter)models}. 

Firstly, state-of-the-art tools are complex and often heavily optimized. 
Their code is hard to understand and difficult to automatically verify.
Therefore, it is reasonable to distrust their answers. 
But if proofs or (counter)models are produced, 
they can serve as independently checkable certificates of 
correctness for the provided answers. 

Secondly, for most applications, a \emph{yes/no} is inherently insufficient. We often already known in advance whether a problem is expected to be satisfiable or unsatisfiable, and we want more than just a confirmation of this expectation. For satisfiable formulas, the desired information is encoded in the model; for valid formulas, it is contained in the proof. In case the expectation was wrong, refutations and countermodels can be very helpful to explain issues in the encoding of the problem, in order to correct and refine it.

Proofs and refutations can been used, for example, to extract unsatisfiable cores \cite{ToDo}, interpolants \cite{ToDo}, Herbrand disjunctions \cite{ToDo}, linguistic implicatures \cite{ToDo} \ldots

Although current automated deduction tools are very efficient at finding proofs, they do not necessarily find the best proofs. {\skeptik}\footnote{\url{http://github.com/Paradoxika/Skeptik/}} aims precisely at efficiently analyzing proofs, in order to compress and improve them by eliminating their redundancies.


\paragraph{Related Work:} \texttt{CERes}\footnote{\url{http://www.logic.at/ceres}} \cite{ToDo} is another proof transformation system, especialized in cut-elimination for classical first-order sequent calculus. It was replaced by \texttt{GAPT}\footnote{\url{http://code.google.com/p/gapt/}} \cite{ToDo}, extended with cut-introduction techniques \cite{ToDo}. \texttt{MINLOG}\footnote{\url{http://www.mathematik.uni-muenchen.de/~logik/minlog/}} \cite{ToDo} extracts functional programs from proofs, employing a refined A-translation for functional proofs.


\newcommand{\com}[1]{\texttt{#1}}

\section{Installation}

{\skeptik} is implemented in Scala and runs on the java virtual machine (JVM). Therefore, Java\footnote{\url{https://www.java.com/}} must be installed. The easiest way to download {\skeptik} is via \texttt{git}\footnote{\url{http://git-scm.com/}}, by executing \com{git clone git@github.com:Paradoxika/Skeptik.git} in the folder where {\skeptik} should be downloaded. It is helpful to install \texttt{SBT}\footnote{\url{http://www.scala-sbt.org/}}, a build tool that automatically downloads all compilers and libraries on which {\skeptik} depends. To compile, build and package {\skeptik}, run \com{sbt one-jar} in {\skeptik}'s home folder. This generates a jar file that can be executed like any other Java jar file.

\texttt{SBT} and Scala programs may need a lot of memory for compilation and execution. If out-of-memory problems occur, the JVM's maximum available memory can be increased by executing \com{export JAVA\_TOOL\_OPTIONS=``-Xmx1024m''} in the terminal.


\section{Usage}

The command \com{java -jar skeptik.jar --help} displays a help message explaining how to use {\skeptik}. For example, to compress the proof ``eq\_diamond9.smt2'' using the algorithm \texttt{RPI} and write the compressed proof using the `smt2' proof format, the following command should be executed: \com{java -jar skeptik.jar -a RPI -f smt2 examples/proofs/VeriT/eq\_diamond9.smt2}. {\skeptik} can be called with an arbitrary number of algorithms and proofs. The following command would compress the proofs ``p1.smt2'' and ``p2.smt2'' with two algorithms each (\texttt{RP} and a sequential composition of \texttt{DAGify}, \texttt{RPI} and \texttt{LU}): \\
\com{java -jar skeptik.jar -a RP -a (DAGify*RPI*LU) p1.smt2 p2.smt2} 


\newcommand{\class}[1]{\texttt{#1}}

\section{Implementation Details}

In {\skeptik} every logical expression is a simply typed lambda expression, implemented by the abstract class \class{E} with concrete subclasses \class{Var}, \class{App} and \class{Abs} for, respectively, \emph{variables}, \emph{applications} and \emph{abstractions}. Scala's \emph{case classes} are used to make \class{E} behave like an algebraic datatype with (pattern-matchable) constructors \class{Var}, \class{App} and \class{Abs} \`{a} la functional programming.

{\skeptik} is flexible w.r.t. the underlying proof calculus. Every proof node is an instance of the abstract class \class{ProofNode} and must contain a judgment of some subclass \class{J} of \class{Judgment} and a (possibly empty) collection of premises (which are other proof nodes). A proof is a directed acyclic graph of proof nodes; it is implemented as the class \class{Proof}, which provides higher-order methods for traversing proofs. Thanks to Scala's syntax conventions, these methods can be used as an internal domain specific language that integrates harmoniously with the scala language itself.

A proof calculus is a collection of inference rules, implemented as concrete subclasses of \class{ProofNode}. In particular, the main inference rules of the propositional resolution calculus used for representing proofs generated by sat- and smt-solvers are \class{Axiom}, \class{R} (for resolution). They use the class \class{Sequent} (a subclass of \class{Judgment}) to represent clauses.

The classes for expressions and proof nodes are small and correctness conditions are checked during object construction. Once constructed, they cannot be changed, because they are \emph{immutable}. Hence, guarantees that incorrect expressions and proofs cannot result from the transformations performed by {\skeptik}. Auxiliary functionality is not implemented in the classes but in their homonymous \emph{companion objects}. Therefore, even though {\skeptik} has more than 21000 lines of code, its most critical core data structures are less than a few hundred lines long.




\subsection{Organization of the Code}

ToDo: brief description of the package structure






\section{Supported Proof Formats}
\label{sec:ProofFormats}

Scala's combinator parsing library makes it easy to implement parsers for various proof formats. {\skeptik} uses the extension of a file to  determine its proof format. The currently available proof formats are:


\paragraph{The SMT-Lib Proof Format: }

{\veriT}

ToDo (Bruno)


\paragraph{TraceCheck Format}

ToDo (Andreas)

ToDo: RUP

cite Armin Biere's sat-solver


\paragraph{{\skeptik}'s own Proof Format}

ToDo (Joseph)

ToDo: with deletion


\section{Proof Compression Algorithms}

Most algorithms for the compression of propositional resolution proofs generated by sat-solvers and smt-solvers described in the literature are available in {\skeptik}. They are shortly described below:


\paragraph{RecyclePivots} (\algo{RP}) \cite{RP08,RP11} compresses a proof by partially regularizing it. A proof is \emph{irregular} \cite{Tseitin} some pivot is resolved more than once on some path
from the root to a leaf.



\paragraph{RecyclePivotsWithIntersection} (\algo{RPI}): 



, Tseitin outlines a first kind of proof redundancy with the concept of
\emph{irregular proof}. A proof is irregular if  But Goerdt \cite{Goerdt} proved that the corresponding regular proof of an
irregular proof may be exponentialy bigger than the original. Therefore, only \emph{partial}
regularisation algorithm can compress a proof.

\algo{RecyclePivots} (\algo{RP})  and \algo{RecyclePivotsWithIntersection}
(\algo{RPI}) \cite{LURPI} are such partial regularisation algorithms. They both compute for each
node a set of \emph{safe literals} and then delete a premise iff its auxiliary literal is safe. The
set of safe literals is constructed recursively from the root of the proof by adding the dual of the
auxiliary literal. The two algorithms only differs for nodes which are premises of more than one
resolution. In that case, \algo{RP} resets the safe literals to the empty set whereas \algo{RPI}
computes the intersection of the safe literals corresponding to each incoming edge.

\subsection{LowerUnits}

Another kind of proof redundancy happens when a node $\node$ appears as premise of many resolutions
on the same pivot $p$. In that case, it would be better to resolve $\node$ on $p$ only once. The
\algo{LowerUnits} (\algo{LU}) algorithm \cite{LURPI} reduces this kind of redundancy by lowering
\emph{units} down to the root of the proof. Units are subproofs whose conclusion consists in a
single literal. Such a subproof can always be lowered down the proof.

\algo{LowerUnits} is of linear time complexity in the length of the proof. It is a very fast
algorithm which perform good compression ratio.

\subsection{LowerUnivalents}

The \algo{LowerUnivalents} algorithm \cite{LUniv} (\algo{LUniv}) generalizes \algo{LowerUnits} by
exploiting the information of the already lowered subproofs and their pivots. Then, it becomes
possible to lower a non-unit node if all its conclusion's literals but one can be deleted by the
already lowered subproofs. Moreover, the lowered pivots are safe literals and thus some partial
regularization can be achieved simultaneously.

In \skeptik, \algo{LUniv} has been implemented as a replacement for the \algo{fixProof} function
used by some algorithms to reconstruct a proof after deletions. Therefore, non-sequential
combinations of those algorithms with \algo{LUniv} are easy to implement. For instance,
\algo{LUnivRPI} is a non-sequential combination of \algo{LUniv} after \algo{RPI}. This latter
algorithm is currently one the best trade-off between compression time and compression ratio
\cite{LUniv}.

\subsection{RPI[3]LU and RPI[3]LUniv}

\algo{RPI[3]LU} and \algo{RPI[3]LUniv} are non-sequential combinations of \algo{RPI} after \algo{LU}
and \algo{LUniv}. They consist in three traversals. The first traversal collects subproofs to be
lowered down the proof. The second traversal computes the sets of safe literals for each node,
taking into account the subproofs marked for being lowered. The last traversal actualy compress the
proof by removing redundant branches and lowering subproofs.

These algorithm are optimizations of the corresponding sequential compositions, achieving the
same compression ratio in less time. 

\subsection{ReduceAndReconstruct}

The \algo{ReduceAndReconstruct} (\algo{RedRec}) approach consists in applying local transformation
rules to each node. The set of local rules presented in \cite{RedRec} are sufficient to emulate any
other compression algorithm. Unfortunately, the proposed heuristic only consists in trying to apply
each rule (with given priorities) to each node in a top-down traversal. For this heuristic to be
efficient, the process has to be repeated many times. The resulting algorithm can achieve very good
compression ratio if run long enough.

As this algorithm allows experimentations in the local transformation rules, the heuristic to apply
them and the termination conditions, its implementation in \skeptik is very modular. Each component
is defined independently and a convenient framework allows to combine them as desired. A handful of
alternative local transformation rules and termination conditions have been implemented too.

\subsection{Split}

The Split \cite{CottonSplit} algorithm is a technique to lower pivot variables in a proof.\\
From a proof with conclusion $C$, two proofs with conclusion $v \vee C$ and $\neg{v} \vee C$ are constructed,  where the variable $v$ is chosen heuristically.\\
In a first step the positive/negative premises of resolvents with pivot $v$ are removed from the proof.
Afterwads the proof is fixed, by traversing it top-down and fixing each proof node.
A proof node is fixed by either replacing it by one of or resolving the fixed premises.\\ %I'm not sure if the end of this sentence is elegant or unreadable
The roots of the resulting proofs are resolved, using $v$ as pivot, to obtain a new proof of $C$.\\
The time-complexity of this algorithm is linear in the proof length %this should be true, right? but the original paper doesn't state this
and it suits very well for repeated application. This can be done iteratively or recursively. Also multiple variables can be chosen in advance. 
All these variants are implemented into Skeptik.

\subsection{TautologyElimination}

\subsection{StructuralHashing}

\subsection{DAGification}

\subsection{Subsumption}

Subsumption based algorithms use, as the name implies, the subsumption relation on clauses for compressing a proof. A clause $C_1$ subsumes a clause $C_2$ \textit{iff} every literal occuring in $C_1$ also occurs in $C_2$.
The general goal is to replace a clause $C$ by another clause $D$, used elsewhere in the proof, that are such that $C$ subsumes $D$.
There are three subsumption based compression algorithms implemented into Skeptik.\\
\textbf{Top-down Subsumption} searches for subsumed clauses among all clauses visited earlier in a top-down traversal. The time-complexity of this algorithm is worst case quadratic in the number of proof nodes.\\
\textbf{Bottom-up Subsumption} searches for subsumed clauses among all clauses visited earlier in a bottom-up traversal. A subsumed clause $D$ can only replace a clause $C$, if $D$ is not an ancestor of $C$ in the graph representing the proof. Bottom-up Subsumption has the same time-complexity as Top-down Subsumption, but the additional ancestor-check makes it slower in practise.\\
\textbf{RecycleUnits} \cite{RP11} is a special case of Bottom-up Subsumption that only searches for subsumed clauses that are unit (i.e. contain only one literal). This algorithm has a worst case time-complexity that is quadratic in the number of unit clauses.

\subsection{Pebbling}

Pebbling algorithms compress proofs in the \textbf{space measure}, as opposed to the usual length compression. The space measure of a proof indicates how many proof nodes maximally have to be kept in memory, while reading the proof, simultaneously.\\
This measure is closely related to the \textbf{Black Pebbling Game} \cite{gilbert1980pebbling}, which lends its name to the algorithms described here. A strategy for this game directly corresponds to a topological node ordering, i.e. an ordering of nodes such that every premise of each node is lower than the node itself, combined with deletion information, i.e. extra lines in the proof output indicating that a node can be deleted from memory. An optimal strategy for the Black Pebbling Game would therefore result in optimal space compression of the proof. However, as shown in \cite{gilbert1980pebbling}, finding the optimal solution is PSPACE-complete and therefore not a feasable approach for this program.\\
To obtain algorithms that have an acceptable runtime greedy heuristics for finding a good node ordering are used.\\
\textbf{Top-down pebbling} directly corresponds to playing the game with limited information on how the proof looks. At every point there are nodes which can be pebbled (initially these are only the axioms). Using information from these nodes and their children nodes, it is decided which node to pebble next, which can make other nodes pebblable. This is done until the root node is pebbled. Unfortunately the lack of knowledge about the structure of the proof often results in bad space compression.\\
\textbf{Bottom-up pebbling} constructs a node ordering by visiting proof nodes and their premises recursively, starting from the root node. At every node it is chosen heuristically in what order its premises are visited. After all premises are visited, the node is added to the oder.



\section{Conclusions and Future Work}

ToDo: limitation: memory consumption in Scala
underlying symbols are strings

New proof formats on demand

extension to first-order

contextual natural deduction

cut-introduction

\bibliographystyle{splncs}
\bibliography{biblio}


\end{document}

% vim: tw=100
