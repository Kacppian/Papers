\documentclass{llncs}

\usepackage{xcolor}
\usepackage{enumitem,amsmath,amssymb}
\usepackage{breakurl}    % used for \url and \burl
\usepackage[linesnumbered,boxed,noline,noend]{algorithm2e}
\def\defaultHypSeparation{\hskip.1in}

\usepackage{tikz}
\usepackage{subfig}
\usepackage{array,booktabs,multirow}
\usepackage{placeins}

\usepackage{logictools}
\usepackage{prooftheory}
\usepackage{comment}
\usepackage{mathenvironments}
\usepackage{drawproof}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}

\newcommand{\defeq}{\mathrel{\mathop:}=}
\newcommand{\eqdef}{\mathrel{\mathop=}:}

\title{Pebbling}

\author{
  Andreas Fellner\inst{1}
  \thanks{Supported by the Google Summer of Code 2013 program.}
  \and 
  Bruno Woltzenlogel Paleo\inst{2}
  \thanks{Supported by the Austrian Science Fund, project P24300.}
}

\authorrunning{A.\~Fellner \and B.\~Woltzenlogel Paleo}

\institute{
  Vienna University of Technology \\
  \email{fellner.a@gmail.com}
  \and 
  Vienna University of Technology \\
  \email{bruno@logic.at}
}




\begin{document}

\maketitle


\begin{abstract}
Bruno
\end{abstract}

\setcounter{footnote}{0}

\section{Introduction}

Bruno

\section{Introduction}
Bruno

Propositional Resolution proofs usually are huge. So huge that when processing such proofs memory limits are reached and exceeded.
%A proof is only good if it is verifiable.
%Software that verifies the correctness of a proof has only got a limited amount of memory that it can allocate. 
%\Skeptik and similar pieces of software provide compression methods that reduce the number of proof nodes and therefore the required memory.
%Additionally when reading a proof incrementally, one can never be sure if a proof node that lies in memory will be needed later in the proof.
However not the whole proof needs to be kept in memory. 
Proof nodes that are not used further on can be removed from memory.
Information when to remove which node can be added as extra lines in the proof output.
%Such deletion information can be added to the proof when writing it to a file.
The maximum number of proof nodes that have to be kept in memory at once using deletion information is called the space measure.
This measure is closely related to the Black Pebbling Game \cite{kasai1979classes,gilbert1980pebbling}.
A strategy for this game corresponds to a topological ordering of the proof nodes plus deletion information.
Finding an optimal strategy for the Black Pebbling Game is PSPACE-complete \cite{gilbert1980pebbling} and therefore not a feasible approach to compress big proofs.
This paper investigates heuristic approaches to the problem.

\section{Propositional Resolution Calculus}

A \emph{literal} is a propositional variable or the negation of a propositional variable. The
\emph{complement} of a literal $\ell$ is denoted $\dual{\ell}$ (i.e. for any propositional variable $p$,
$\dual{p} = \neg p$ and $\dual{\neg p} = p$). The set of all literals is denoted $\mathcal{L}$. A
\emph{clause} is a set of literals. $\bot$ denotes the \emph{empty clause}.


\newcommand{\axiom}[1]{\widehat{#1}}
\newcommand{\n}{v}
\newcommand{\raiz}[1]{\rho(#1)}

\begin{definition}[Proof] 
\label{def:proof}
A directed acyclic graph $\langle V, E, \clause \rangle$, where $V$ is a set of nodes and $E$ is a
set of edges labeled by literals (i.e. $E \subset V \times \mathcal{L} \times V$ and $\n_1
\xrightarrow{\ell} \n_2$ denotes an edge from node $\n_1$ to node $\n_2$ labeled by $\ell$), is a
proof of a clause $\clause$ iff it is inductively constructible according to the following cases:
%
\begin{enumerate}
  \item If $\Gamma$ is a clause, $\axiom{\Gamma}$ denotes some proof $\langle \{ \n \}, \emptyset ,
    \Gamma \rangle$, where $\n$ is a new node.
  \item If $\psi_L$ is a proof $\langle V_L, E_L, \clause_L \rangle$ and
    $\psi_R$ is a proof $\langle V_R, E_R, \clause_R \rangle$ and $\ell$ is a literal such that
    $\dual{\ell} \in \clause_L$ and $\ell \in \clause_R$, then
    $\psi_L \odot_\ell \psi_R$ denotes a proof $\langle V, E, \Gamma \rangle$ s.t.
    \begin{align*}
      V &= V_L \cup V_R \cup \{\n \} \\
      E &= E_L \cup E_R \cup
                    \left\{ \n \xrightarrow{\dual{\ell}} \raiz{\psi_L}, \n \xrightarrow{\ell} \raiz{\psi_R} \right\} \\
     \Gamma &= \left( \clause_L \setminus \left\{ \dual{\ell} \right\} \right) \cup \left( \clause_R
                    \setminus \left\{ \ell \right\} \right)
    \end{align*}
    where $\n$ is a new node and $\raiz{\varphi}$ denotes the root node of $\varphi$.
  \qed
\end{enumerate}
\end{definition}


\newcommand{\Vertices}[1]{V_{#1}}
\newcommand{\Edges}[1]{E_{#1}}
\newcommand{\Conclusion}[1]{\clause_{#1}}
\newcommand{\Premises}[2]{P_{#1}^{#2}}
\newcommand{\Children}[2]{C_{#1}^{#2}}

\noindent
If $\psi = \varphi_L \odot_{\ell} \varphi_R$, then $\varphi_L$ and $\varphi_R$ are \emph{direct
subproofs} of $\psi$ and $\psi$ is a \emph{child} of both $\varphi_L$ and $\varphi_R$. The
transitive closure of the direct subproof relation is the \emph{subproof} relation. A subproof which
has no direct subproof is an \emph{axiom} of the proof. 
%I will probably never need the following paragraph in this paper
Contrary to the usual proof
theoretic conventions but following the actual implementation of the data structures used by
\PebblingAlgorithms, edges are directed from children (resolvents) to their parents (premises).
%
$\Vertices{\psi}$, $\Edges{\psi}$ and $\Conclusion{\psi}$
denote, respectively, the nodes, edges and proved clause (conclusion) of $\psi$.\\
Let $\n$ be a node in $\psi$, then $\Premises{\n}{\psi}$ denotes its premises and $\Children{\n}{\psi}$ its children in $\psi$.

\begin{definition}[Topological Order]
\label{def:topological order}
A topological order of a proof $\psi$ is a linear order relation $<_T$ on $\Vertices{\psi}$, such that 
$$
\text{for all } \n \in \Vertices{\psi} \text{, for all } p \in \Premises{\n}{\psi} \text{ hold }\\
p <_T v
$$
\end{definition}
A topological order $<_T$ can be represented by a sequence of proof nodes $S = (s_1,\ldots,\s_n)$, by defining $<_T \defeq \{(s_i,s_j) \mid 1 \leq i < j \leq n\}$.\\
Note that there are proofs, for which exponentially many different topological orders exists: Consider a proof with $n$ axioms. Every permutation of these axioms can be extended to a sequence which represent a topological order. There are $n!$ such permutations, so the overall number of topological orders is at least exponential in $n$.\\
This means that when looking for a good topological order w.r.t. any characteristic, enumeration is not a feasible option.

By loading nodes into memory following a topological order, proofs can be traversed in a top-down fashion, for example to check the correctness of the proof.\\
Not all nodes have to be kept in memory when doing such a traversal. As soon as all nodes $p \in \Children{\n}{\psi}$ have been loaded into memory, $\n$ can be removed from memory,
since it will not be needed further on.

\begin{definition}[Space measure]
\label{def:space measure}
The space measure $sp_{<_T}(\psi)$ of a proof $\psi$ is the maximum number of nodes that have to be kept in memory, when traversing $\psi$ in a top-down fashion, following the topological order $<_T$.
\end{definition}

\section{Pebbling Game}

\section{Greedy Pebbling Algorithms}

The following two algorithms search for a topological order of a proof in a bottom-up fashion, using a predefined heuristic.

\SetKwFunction{KwVisit}{visit}

\begin{algorithm}[h]
  \KwIn{a proof $\psi$ with root node $r$}
  \KwOut{a topological order $<_T$ of $\psi$ represented by a sequence of nodes}
  \BlankLine

	$S$ is the empty sequence\;
	$V = \emptyset$\;
	\Return \KwVisit{$\psi$,$r$,$V$,$S$}\;

  \caption[.]{\FuncSty{Bottom-up Pebbling}}
  \label{algo:BUpebbling}
\end{algorithm}

\begin{algorithm}[h]
  \KwIn{a proof $\psi$, a node $\n$}
	\KwIn{a set of visited nodes $V$} 
	\KwIn{initial sequence of nodes $S$}
  \KwOut{a sequence of nodes}
	
	$V_1 = V \cup \{\n\}$\;
	$D = \Premises{\n}{\psi} \setminus V$\;
	$S_1 = S$
	
  \While{$D$ is not empty}{
    choose $p \in D$ heuristically\;
		$D = D \setminus \n$\;
		$S_1 = S_1 \oplus visit(\psi,\n,V,S)$\;
  }
	
	\Return $S_1 \oplus \n$\;
	
  \caption[.]{\FuncSty{visit}}
  \label{algo:visit}
\end{algorithm}

The following algorithm searches for a topological order of a proof in a top-down fashion, using a predefined heuristic.

\begin{algorithm}[h]
  \KwIn{a proof $\psi$}
  \KwOut{a topological order $<_T$ of $\psi$ represented by a sequence of nodes}
	
	$S$ is the empty sequence\;
	$A = \{p \in \Verices{\psi} \mid p \text{ is an axiom}\}$\;
	
  \While{$A$ is not empty}{
    choose $\n \in A$ heuristically\;
		\For{\KwSty{each} $c \in \Children{\n}{\psi}$}{
			\If{$\forall p \in \Premises{c}{\psi}: p \in S$}
					{$A = A \cup \{c\}$\;}
					}
		$A = A \setminus \{\n\}$\;
		$S = S \oplus \n$\;
  }
	
	\Return $S$\;
	
  \caption[.]{\FuncSty{top-down pebbling}}
  \label{algo:TDpebbling}
\end{algorithm}

\subsection{Last Child Heuristic}

\subsection{Node Distance Heuristic}

\subsection{Number of Children Heuristic}




\section{Experiments} \label{sec:exp}

{\LowerUnivalents} and {\LUnivRPI} have been implemented in the functional programming
language Scala\footnote{\url{http://www.scala-lang.org/}} as part of the \skeptik
library\footnote{\url{https://github.com/Paradoxika/Skeptik}}. {\LowerUnivalents} has been implemented as a
recursive \FuncSty{delete} improvement.

The algorithms have been applied to 5\,059 proofs produced by the SMT-solver
{\veriT}\footnote{\url{http://www.verit-solver.org/}} on unsatisfiable benchmarks from the
SMT-Lib\footnote{\url{http://www.smtlib.org/}}.  The details on the number of proofs per SMT category
are shown in Table \ref{tab:benchmarks}.  The proofs were translated into pure resolution proofs by
considering every non-resolution inference as an axiom.

\begin{table}[tb]
  \caption{Number of proofs per benchmark category}
  \label{tab:benchmarks}
  \centering
  \begin{tabular}{lr}
    \toprule
    Benchmark~ &  Number \\
    Category       & ~of Proofs \\
    \midrule
    QF\_UF      & 3907 \\
    QF\_IDL     &  475 \\
    QF\_LIA     &  385 \\
    QF\_UFIDL   &  156 \\
    QF\_UFLIA   &  106 \\
    QF\_RDL     &   30 \\
    \bottomrule
  \end{tabular}
\end{table}

The experiment compared the following algorithms:
\begin{description}
  \item[LU:] the {\LowerUnits} algorithm from \cite{LURPI};
  \item[LUniv:] the {\LowerUnivalents} algorithm;
  \item[RPILU:] a non-sequential combination of {\RPI} after {\LowerUnits};
  \item[RPILUniv:] a non-sequential combination of {\RPI} after {\LowerUnivalents};
  \item[LU.RPI:] the sequential composition of {\LowerUnits} after {\RPI};
  \item[LUnivRPI:] the non-sequential combination of {\LowerUnivalents} after {\RPI} as described in Sect. \ref{sec:LUnivRPI};
  \item[RPI:] the {\RecyclePivotsIntersection} from \cite{LURPI};
  \item[Split:] Cotton's \texttt{Split} algorithm (\cite{CottonSplit});
  \item[RedRec:] the {\ReduceReconstruct} algorithm from \cite{RedRec};
  \item[Best RPILU/LU.RPI:] which performs both \texttt{RPILU} and \texttt{LU.RPI} and chooses the smallest resulting compressed proof;
  \item[Best RPILU/LUnivRPI:] which performs \texttt{RPILU} and \texttt{LUnivRPI} and chooses the smallest resulting
    compressed proof.
\end{description}

For each of these algorithms, the time needed to compress the proof along with the number of nodes
and the number of axioms (i.e. \emph{unsat core} size) have been measured. Raw data of the
experiment can be downloaded from the web\footnote{\url{http://www.matabio.net/skeptik/LUniv/experiments/}}.

The experiments were executed on the Vienna Scientific Cluster\footnote{\url{http://vsc.ac.at/}}
VSC\nobreakdash-2. Each algorithm was executed in a single core and had up to 16 GB of memory available. This amount of memory has been useful to compress the biggest proofs (with more than $10^6$ nodes).


The overall results of the experiments are shown in Table \ref{tab:average}. The compression ratios
in the second column are computed according to formula (\ref{eq:compression}), in which $\psi$
ranges over all the proofs in the benchmark and $\psi'$ ranges over the corresponding compressed
proofs.
\begin{equation} \label{eq:compression}
  1 - \frac{ \sum {|\Vertices{\psi'}|} }{ \sum {|\Vertices{\psi}|} }
\end{equation}
The unsat core compression ratios are computed in the same way, but using the number of axioms instead of
the number of nodes. The speeds on the fourth column are computed according to formula
(\ref{eq:speed}) in which $d_{\psi}$ is the duration in milliseconds of $\psi$'s compression by a
given algorithm.
\begin{equation} \label{eq:speed}
  \frac{ \sum {|\Vertices{\psi}|} }{ \sum {d_{\psi}} }
\end{equation}

For the \texttt{Split} and \texttt{RedRec} algorithms, which must be repeated, a timeout has
been fixed so that the speed is about 3 nodes per millisecond. 


\begin{table}[tb]
  \caption{Total compression ratios}
  \label{tab:average}
  \centering
  \begin{tabular}{lrrr}
    \toprule
    \multirow{2}{*}{Algorithm} & \multirow{2}{*}{Compression} & Unsat Core    & \phantom{.........}\multirow{2}{*}{Speed} \\
                                             &                                               & \phantom{...}Compression &        \\
    \midrule
    LU                &  7.5 \% &  0.0 \% & 22.4 n/ms \\
    LUniv             &  8.0 \% &  0.8 \% & 20.4 n/ms \\
    RPILU             & 22.0 \% &  3.6 \% &  7.4 n/ms \\
    RPILUniv          & 22.1 \% &  3.6 \% &  6.5 n/ms \\
    LU.RPI            & 21.7 \% &  3.1 \% & 15.1 n/ms \\
    LUnivRPI          & 22.0 \% &  3.6 \% & 17.8 n/ms \\
    RPI               & 17.8 \% &  3.1 \% & 31.3 n/ms \\
    Split             & 21.0 \% &  0.8 \% &  2.9 n/ms \\
    RedRec            & 26.4 \% &  0.4 \% &  2.9 n/ms \\
    Best RPILU/LU.RPI       & 22.0 \% &  3.7 \% &  5.0 n/ms \\
    Best RPILU/LUnivRPI & 22.2 \% &  3.7 \% &  5.2 n/ms \\
    \bottomrule
  \end{tabular}
\end{table}

\newcommand{\va}[1]{\ensuremath{v_{\text{#1}}}}

Figure \ref{fig:LU} shows the comparison of {\LowerUnits} with {\LowerUnivalents}. Subfigures (a) and (b) are scatter plots where each dot represents a single benchmark proof. 
Subfigure (c) is a histogram showing, in the vertical axis, the proportion of proofs having \emph{(normalized) compression ratio difference} within the intervals showed in the horizontal axis. This difference is computed using formula (\ref{eq:histogram}) with \va{LU}
and \va{LUniv} being the compression ratios obtained respectively by {\LowerUnits} and
{\LowerUnivalents}.
\begin{equation} \label{eq:histogram}
  \frac { \va{LU} - \va{LUniv} }{ \frac{\va{LU} + \va{LUniv}}{2} }
\end{equation}
The number of proofs for which $\va{LU} = \va{LUniv}$ is not displayed in the histogram.
The \emph{(normalized) duration differences} in subfigure (d) are computed using the same formula~(\ref{eq:histogram}) but
with \va{LU} and \va{LUniv} being the time taken to compress the proof by {\LowerUnits} and
{\LowerUnivalents} respectively.



As expected, {\LowerUnivalents} always compresses more than {\LowerUnits} (subfigure (a)) at the expense of a longer
computation (subfigure (d)). And even if the compression gain is low on average (as noticeable in Table \ref{tab:average}), subfigure (a) shows that {\LowerUnivalents} compresses some proofs significantly more than {\LowerUnits}.

It has to be noticed that \veriT already does its best to produce compact proofs. In particular,
a forward subsumption algorithm is applied, which results in proofs not having two different subproofs
with the same conclusion. This results in {\LowerUnits} being unable to reduce unsat core.
But as {\LowerUnivalents} lowers non-unit subproofs and performs some partial regularization, it
achieves some unsat core reduction, as noticeable in subfigure (b).

The comparison of the sequential \texttt{LU.RPI} with the non-sequential {\LUnivRPI} shown in Fig.
\ref{fig:LUnivRPI} outlines the ability of {\LowerUnivalents} to be efficiently combined with other
algorithms. Not only compression ratios are improved but {\LUnivRPI} is faster than the sequential
composition for more than 80 \% of the proofs.







\section{Conclusions and Future Work}

Bruno


\vspace{-10pt}
\paragraph{Acknowledgments:}



\bibliographystyle{splncs}
\bibliography{biblio}


\end{document}

% vim: tw=100
